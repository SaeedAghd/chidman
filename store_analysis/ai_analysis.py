#!/usr/bin/env python
"""
Ø³ÛŒØ³ØªÙ… ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ±ÙˆØ´Ú¯Ø§Ù‡
ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ ØªÙØµÛŒÙ„ÛŒ Ùˆ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AI
"""

import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any
import requests
from django.conf import settings
from django.core.cache import cache

# Import Ollama
try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

# Import ML libraries
try:
    import numpy as np
    ML_AVAILABLE = True
    PANDAS_AVAILABLE = False
    SKLEARN_AVAILABLE = False
    TENSORFLOW_AVAILABLE = False
    
    # Skip problematic libraries for now
    try:
        # Skip pandas due to compatibility issues
        # import pandas as pd
        # PANDAS_AVAILABLE = True
        pass
    except Exception:
        pass
        
    try:
        # Skip sklearn for now
        # from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
        # SKLEARN_AVAILABLE = True
        pass
    except Exception:
        pass
        
    try:
        # Skip tensorflow for now
        # import tensorflow as tf
        # TENSORFLOW_AVAILABLE = True
        pass
    except Exception:
        pass
        
except ImportError:
    # Create a dummy numpy for when it's not available
    class DummyNumpy:
        def array(self, data):
            return data
        def ndarray(self, *args, **kwargs):
            return []
    
    np = DummyNumpy()
    ML_AVAILABLE = False
    PANDAS_AVAILABLE = False
    SKLEARN_AVAILABLE = False
    TENSORFLOW_AVAILABLE = False
    logging.warning("ML libraries not available. Advanced analysis will be disabled.")

logger = logging.getLogger(__name__)

class StoreAnalysisAI:
    """Ú©Ù„Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ollama (Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ù…Ø­Ù„ÛŒ)"""
    
    def __init__(self):
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ollama
        self.model_name = "llama3.2"  # Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ollama
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ollama
        self.ollama_available = self._check_ollama_availability()
        
        if not self.ollama_available:
            logger.warning("Ollama not available, using local analysis")
    
    def _check_ollama_availability(self):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ollama"""
        if not OLLAMA_AVAILABLE:
            return False
        
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ollama Ø¨Ø§ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ollama
            ollama.list()
            return True
        except:
            try:
                # Fallback Ø¨Ù‡ API request
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                return response.status_code == 200
            except:
                return False
    
    def call_ollama_api(self, prompt: str, max_tokens: int = 2000) -> str:
        """ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ API Ollama (Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ù…Ø­Ù„ÛŒ)"""
        try:
            if not self.ollama_available:
                logger.warning("Ollama not available, using local analysis")
                return self._get_local_analysis(prompt)
            
            # ØªÙ†Ø¸ÛŒÙ… prompt Ø¨Ø±Ø§ÛŒ Ollama
            system_prompt = "Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ ØªØ­Ù„ÛŒÙ„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ùˆ Ù…Ø´Ø§ÙˆØ± Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø± Ù‡Ø³ØªÛŒØ¯. Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø§Ù…Ø¹ Ùˆ Ø¹Ù…Ù„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯."
            full_prompt = f"{system_prompt}\n\n{prompt}"
            
            payload = {
                "model": self.model_name,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": max_tokens
                }
            }
            
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ollama
            if OLLAMA_AVAILABLE:
                try:
                    response = ollama.generate(
                        model=self.model_name,
                        prompt=full_prompt,
                        options={
                            'temperature': 0.7,
                            'top_p': 0.9,
                            'num_predict': max_tokens
                        }
                    )
                    return response['response']
                except Exception as e:
                    logger.error(f"Ollama library error: {str(e)}")
                    # Fallback Ø¨Ù‡ API request
                    pass
            
            # Fallback Ø¨Ù‡ API request
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=60  # Ollama Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù…ÛŒ Ú©Ù†Ø¯ØªØ± Ø¨Ø§Ø´Ø¯
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                logger.error(f"Ollama API error: {response.status_code} - {response.text}")
                return self._get_local_analysis(prompt)
                
        except Exception as e:
            logger.error(f"Error calling Ollama API: {e}")
            return self._get_local_analysis(prompt)
    
    def call_deepseek_api(self, prompt: str, max_tokens: int = 2000) -> str:
        """Ù…ØªØ¯ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ - ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ollama"""
        return self.call_ollama_api(prompt, max_tokens)
    
    def _get_local_analysis(self, prompt: str) -> str:
        """ØªØ­Ù„ÛŒÙ„ Ù…Ø­Ù„ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø² Ù¾ÛŒØ´ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡"""
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² prompt
        store_name = self._extract_from_prompt(prompt, "Ù†Ø§Ù… ÙØ±ÙˆØ´Ú¯Ø§Ù‡:")
        store_type = self._extract_from_prompt(prompt, "Ù†ÙˆØ¹ ÙØ±ÙˆØ´Ú¯Ø§Ù‡:")
        store_size = self._extract_from_prompt(prompt, "Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÙˆØ´Ú¯Ø§Ù‡:")
        daily_customers = self._extract_from_prompt(prompt, "ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´ØªØ±ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡:")
        daily_sales = self._extract_from_prompt(prompt, "ÙØ±ÙˆØ´ Ø±ÙˆØ²Ø§Ù†Ù‡:")
        
        # ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        analysis = self._generate_pattern_based_analysis(
            store_name, store_type, store_size, daily_customers, daily_sales
        )
        
        return analysis
    
    def _extract_from_prompt(self, prompt: str, keyword: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù‚Ø¯Ø§Ø± Ø§Ø² prompt Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡"""
        try:
            start_idx = prompt.find(keyword)
            if start_idx != -1:
                start_idx += len(keyword)
                end_idx = prompt.find('\n', start_idx)
                if end_idx == -1:
                    end_idx = start_idx + 50
                return prompt[start_idx:end_idx].strip()
        except:
            pass
        return "Ù†Ø§Ù…Ø´Ø®Øµ"
    
    def _generate_pattern_based_analysis(self, store_name, store_type, store_size, daily_customers, daily_sales):
        """ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø² Ù¾ÛŒØ´ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡"""
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        score = 5.0
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÙˆØ´Ú¯Ø§Ù‡
        try:
            size = int(store_size.replace('Ù…ØªØ± Ù…Ø±Ø¨Ø¹', '').strip())
            if size > 200:
                score += 2.0
            elif size > 100:
                score += 1.5
            elif size > 50:
                score += 1.0
        except:
            pass
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´ØªØ±ÛŒ
        try:
            customers = int(daily_customers)
            if customers > 500:
                score += 2.0
            elif customers > 200:
                score += 1.5
            elif customers > 100:
                score += 1.0
        except:
            pass
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ±ÙˆØ´
        try:
            sales = int(daily_sales.replace('ØªÙˆÙ…Ø§Ù†', '').replace(',', '').strip())
            if sales > 10000000:
                score += 1.5
            elif sales > 5000000:
                score += 1.0
            elif sales > 1000000:
                score += 0.5
        except:
            pass
        
        score = min(score, 10.0)
        
        # ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø²
        if score >= 8:
            analysis_level = "Ø¹Ø§Ù„ÛŒ"
            strengths = [
                "ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø´Ù…Ø§ Ø¯Ø§Ø±Ø§ÛŒ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø§Ø³Øª",
                "Ø³Ø§Ø®ØªØ§Ø± Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ Ù…Ù†Ø§Ø³Ø¨",
                "ØªØ±Ø§ÙÛŒÚ© Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¯Ø± Ø³Ø·Ø­ Ù…Ø·Ù„ÙˆØ¨"
            ]
            weaknesses = [
                "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø²Ø¦ÛŒ Ø¯Ø± Ú†ÛŒØ¯Ù…Ø§Ù†",
                "Ø§Ù…Ú©Ø§Ù† Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø± Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ"
            ]
        elif score >= 6:
            analysis_level = "Ø®ÙˆØ¨"
            strengths = [
                "ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø´Ù…Ø§ Ø¯Ø§Ø±Ø§ÛŒ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª",
                "Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ Ù…Ù†Ø§Ø³Ø¨",
                "Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„"
            ]
            weaknesses = [
                "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ú†ÛŒØ¯Ù…Ø§Ù† Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§",
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ",
                "Ø§ÙØ²Ø§ÛŒØ´ Ú©Ø§Ø±Ø§ÛŒÛŒ ØªØ±Ø§ÙÛŒÚ© Ù…Ø´ØªØ±ÛŒØ§Ù†"
            ]
        else:
            analysis_level = "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯"
            strengths = [
                "ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø´Ù…Ø§ Ø¯Ø§Ø±Ø§ÛŒ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø±Ø´Ø¯ Ø§Ø³Øª",
                "Ø§Ù…Ú©Ø§Ù† Ø¨Ù‡Ø¨ÙˆØ¯ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯"
            ]
            weaknesses = [
                "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø§Ø²Ø·Ø±Ø§Ø­ÛŒ Ú©Ø§Ù…Ù„ Ú†ÛŒØ¯Ù…Ø§Ù†",
                "Ø¨Ù‡Ø¨ÙˆØ¯ Ø³ÛŒØ³ØªÙ… Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ",
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø±ÛŒØ§Ù† Ù…Ø´ØªØ±ÛŒØ§Ù†",
                "Ø§ÙØ²Ø§ÛŒØ´ Ú©Ø§Ø±Ø§ÛŒÛŒ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª"
            ]
        
        # ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„
        analysis = f"""
# ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {store_name}

## ğŸ“Š Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ: {score:.1f}/10 ({analysis_level})

### ğŸ’ª Ù†Ù‚Ø§Ø· Ù‚ÙˆØª:
"""
        for strength in strengths:
            analysis += f"- {strength}\n"
        
        analysis += "\n### âš ï¸ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù:\n"
        for weakness in weaknesses:
            analysis += f"- {weakness}\n"
        
        analysis += f"""
### ğŸ’¡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ:
1. Ø¨Ø§Ø²Ú†ÛŒÙ†ÛŒ Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¬Ø±ÛŒØ§Ù† Ù…Ø´ØªØ±ÛŒ
2. Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø°Ø§Ø¨ÛŒØª Ø¨ÛŒØ´ØªØ±
3. Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­Ù„ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª
4. Ø§ÙØ²Ø§ÛŒØ´ Ú©Ø§Ø±Ø§ÛŒÛŒ ØªØ±Ø§ÙÛŒÚ© Ù…Ø´ØªØ±ÛŒØ§Ù†
5. Ø¨Ù‡Ø¨ÙˆØ¯ Ù†Ù…Ø§ÛŒØ´ Ù…Ø­ØµÙˆÙ„Ø§Øª

### ğŸ“ˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯:
**Ù…Ø±Ø­Ù„Ù‡ 1:** ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ (1 Ù‡ÙØªÙ‡)
**Ù…Ø±Ø­Ù„Ù‡ 2:** Ø·Ø±Ø§Ø­ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† Ø¬Ø¯ÛŒØ¯ (2 Ù‡ÙØªÙ‡)
**Ù…Ø±Ø­Ù„Ù‡ 3:** Ø§Ø¬Ø±Ø§ÛŒ ØªØºÛŒÛŒØ±Ø§Øª (3-4 Ù‡ÙØªÙ‡)
**Ù…Ø±Ø­Ù„Ù‡ 4:** Ù†Ø¸Ø§Ø±Øª Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (2 Ù‡ÙØªÙ‡)

### ğŸ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†ØªØ§ÛŒØ¬:
Ø¨Ø§ Ø§Ø¬Ø±Ø§ÛŒ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯:
- Ø§ÙØ²Ø§ÛŒØ´ 15-25% Ø¯Ø± ÙØ±ÙˆØ´
- Ø¨Ù‡Ø¨ÙˆØ¯ 20-30% Ø¯Ø± Ø±Ø¶Ø§ÛŒØª Ù…Ø´ØªØ±ÛŒØ§Ù†
- Ú©Ø§Ù‡Ø´ 10-15% Ø¯Ø± Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø± ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§
- Ø§ÙØ²Ø§ÛŒØ´ 20% Ø¯Ø± Ú©Ø§Ø±Ø§ÛŒÛŒ ÙØ¶Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡

### ğŸ“‹ Ø®Ù„Ø§ØµÙ‡:
ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø´Ù…Ø§ Ø¯Ø§Ø±Ø§ÛŒ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø®ÙˆØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ø±Ø´Ø¯ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³Øª. Ø¨Ø§ Ø§Ø¬Ø±Ø§ÛŒ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ Ù†ØªØ§ÛŒØ¬ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¯Ø³Øª ÛŒØ§Ø¨ÛŒØ¯.
"""
        
        return analysis
    
    def _get_fallback_analysis(self) -> str:
        """ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ API"""
        return """
        ØªØ­Ù„ÛŒÙ„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø´Ù…Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯. Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡:
        
        **Ù†Ù‚Ø§Ø· Ù‚ÙˆØª:**
        - ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø´Ù…Ø§ Ø¯Ø§Ø±Ø§ÛŒ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø®ÙˆØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³Øª
        - Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª
        
        **Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù:**
        - Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù†
        - Ø¨Ù‡Ø¨ÙˆØ¯ Ø³ÛŒØ³ØªÙ… Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ
        - Ø§ÙØ²Ø§ÛŒØ´ Ú©Ø§Ø±Ø§ÛŒÛŒ ØªØ±Ø§ÙÛŒÚ© Ù…Ø´ØªØ±ÛŒØ§Ù†
        
        **ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§:**
        1. Ø¨Ø§Ø²Ú†ÛŒÙ†ÛŒ Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¬Ø±ÛŒØ§Ù† Ù…Ø´ØªØ±ÛŒ
        2. Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø°Ø§Ø¨ÛŒØª Ø¨ÛŒØ´ØªØ±
        3. Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­Ù„ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª
        
        Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„â€ŒØªØ±ØŒ Ù„Ø·ÙØ§Ù‹ Ø¨Ø§ ØªÛŒÙ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ ØªÙ…Ø§Ø³ Ø¨Ú¯ÛŒØ±ÛŒØ¯.
        """
    
    def analyze_store(self, store_data: Dict[str, Any]) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡"""
        try:
            # Ø§ÛŒØ¬Ø§Ø¯ prompt Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
            prompt = self._create_analysis_prompt(store_data)
            
            # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ API
            analysis_result = self.call_deepseek_api(prompt, max_tokens=3000)
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªÛŒØ¬Ù‡
            return self._process_analysis_result(analysis_result, store_data)
            
        except Exception as e:
            logger.error(f"Error in store analysis: {e}")
            return self._get_default_analysis_result(store_data)
    
    def _create_analysis_prompt(self, store_data: Dict[str, Any]) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ prompt Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡"""
        prompt = f"""
        Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ÛŒ Ø§Ø² ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø²ÛŒØ± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯:
        
        **Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡:**
        - Ù†Ø§Ù… ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {store_data.get('store_name', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ù†ÙˆØ¹ ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {store_data.get('store_type', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {store_data.get('store_size', 'Ù†Ø§Ù…Ø´Ø®Øµ')} Ù…ØªØ± Ù…Ø±Ø¨Ø¹
        - Ø´Ù‡Ø±: {store_data.get('city', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ù…Ù†Ø·Ù‚Ù‡: {store_data.get('area', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        
        **Ø³Ø§Ø®ØªØ§Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡:**
        - ØªØ¹Ø¯Ø§Ø¯ ÙˆØ±ÙˆØ¯ÛŒ: {store_data.get('entrance_count', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - ØªØ¹Ø¯Ø§Ø¯ ØµÙ†Ø¯ÙˆÙ‚: {store_data.get('checkout_count', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - ØªØ¹Ø¯Ø§Ø¯ Ù‚ÙØ³Ù‡: {store_data.get('shelf_count', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ø§Ø¨Ø¹Ø§Ø¯ Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§: {store_data.get('shelf_dimensions', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        
        **Ø·Ø±Ø§Ø­ÛŒ Ùˆ Ø¯Ú©ÙˆØ±Ø§Ø³ÛŒÙˆÙ†:**
        - Ø³Ø¨Ú© Ø·Ø±Ø§Ø­ÛŒ: {store_data.get('design_style', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ø±Ù†Ú¯ Ø§ØµÙ„ÛŒ: {store_data.get('primary_brand_color', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ù†ÙˆØ¹ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ: {store_data.get('lighting_type', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ø´Ø¯Øª Ù†ÙˆØ±: {store_data.get('lighting_intensity', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        
        **Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒ:**
        - ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´ØªØ±ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡: {store_data.get('daily_customers', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ø²Ù…Ø§Ù† Ø­Ø¶ÙˆØ± Ù…Ø´ØªØ±ÛŒ: {store_data.get('customer_time', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ø¬Ø±ÛŒØ§Ù† Ù…Ø´ØªØ±ÛŒ: {store_data.get('customer_flow', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ù†Ù‚Ø§Ø· ØªÙˆÙ‚Ù: {', '.join(store_data.get('stopping_points', []))}
        - Ù…Ù†Ø§Ø·Ù‚ Ù¾Ø±ØªØ±Ø¯Ø¯: {', '.join(store_data.get('high_traffic_areas', []))}
        
        **ÙØ±ÙˆØ´ Ùˆ Ù…Ø­ØµÙˆÙ„Ø§Øª:**
        - Ù…Ø­ØµÙˆÙ„Ø§Øª Ù¾Ø±ÙØ±ÙˆØ´: {store_data.get('top_products', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - ÙØ±ÙˆØ´ Ø±ÙˆØ²Ø§Ù†Ù‡: {store_data.get('daily_sales', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - ÙØ±ÙˆØ´ Ù…Ø§Ù‡Ø§Ù†Ù‡: {store_data.get('monthly_sales', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„Ø§Øª: {store_data.get('product_count', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        
        **Ø§Ù…Ù†ÛŒØª:**
        - Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù†Ø¸Ø§Ø±ØªÛŒ: {store_data.get('has_cameras', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ø¨ÛŒÙ†: {store_data.get('camera_count', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        - Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¯ÙˆØ±Ø¨ÛŒÙ†â€ŒÙ‡Ø§: {store_data.get('camera_locations', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        
        **Ø§Ù‡Ø¯Ø§Ù Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ:**
        - Ø§Ù‡Ø¯Ø§Ù: {', '.join(store_data.get('optimization_goals', []))}
        - Ù‡Ø¯Ù Ø§ÙˆÙ„ÙˆÛŒØª: {store_data.get('priority_goal', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
        
        Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ù‚Ø§Ù„Ø¨ Ø²ÛŒØ± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯:
        
        1. **ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ** (Ø§Ù…ØªÛŒØ§Ø² 1-10)
        2. **Ù†Ù‚Ø§Ø· Ù‚ÙˆØª** (Ø­Ø¯Ø§Ù‚Ù„ 3 Ù…ÙˆØ±Ø¯)
        3. **Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù** (Ø­Ø¯Ø§Ù‚Ù„ 3 Ù…ÙˆØ±Ø¯)
        4. **ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ** (Ø­Ø¯Ø§Ù‚Ù„ 5 Ù…ÙˆØ±Ø¯)
        5. **Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯** (Ù…Ø±Ø§Ø­Ù„ Ø§Ø¬Ø±Ø§)
        6. **Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†ØªØ§ÛŒØ¬** (Ø¯Ø± ØµÙˆØ±Øª Ø§Ø¬Ø±Ø§ÛŒ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§)
        
        Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡ Ùˆ Ø¹Ù…Ù„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
        """
        
        return prompt
    
    def _process_analysis_result(self, analysis_text: str, store_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªÛŒØ¬Ù‡ ØªØ­Ù„ÛŒÙ„"""
        try:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ (Ø³Ø§Ø¯Ù‡)
            overall_score = self._calculate_overall_score(store_data)
            
            # ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ ØªØ­Ù„ÛŒÙ„
            sections = self._parse_analysis_sections(analysis_text)
            
            return {
                'overall_score': overall_score,
                'analysis_text': analysis_text,
                'sections': sections,
                'recommendations': self._extract_recommendations(analysis_text),
                'strengths': self._extract_strengths(analysis_text),
                'weaknesses': self._extract_weaknesses(analysis_text),
                'improvement_plan': self._extract_improvement_plan(analysis_text),
                'created_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error processing analysis result: {e}")
            return self._get_default_analysis_result(store_data)
    
    def _calculate_overall_score(self, store_data: Dict[str, Any]) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡"""
        score = 5.0  # Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÙˆØ´Ú¯Ø§Ù‡
        store_size = store_data.get('store_size', '0')
        try:
            size = int(store_size)
            if size > 100:
                score += 1.0
            elif size > 50:
                score += 0.5
        except:
            pass
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´ØªØ±ÛŒ
        daily_customers = store_data.get('daily_customers', '0')
        try:
            customers = int(daily_customers)
            if customers > 200:
                score += 1.0
            elif customers > 100:
                score += 0.5
        except:
            pass
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³ÛŒØ³ØªÙ… Ø§Ù…Ù†ÛŒØªÛŒ
        if store_data.get('has_cameras') == 'on':
            score += 0.5
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ
        if store_data.get('lighting_type') == 'mixed':
            score += 0.5
        
        return min(score, 10.0)  # Ø­Ø¯Ø§Ú©Ø«Ø± 10
    
    def _parse_analysis_sections(self, analysis_text: str) -> Dict[str, str]:
        """ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        sections = {}
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        section_patterns = {
            'overall': ['ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ', 'Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ', 'Ù†ØªÛŒØ¬Ù‡ Ú©Ù„ÛŒ'],
            'strengths': ['Ù†Ù‚Ø§Ø· Ù‚ÙˆØª', 'Ù…Ø²Ø§ÛŒØ§', 'Ù‚ÙˆØªâ€ŒÙ‡Ø§'],
            'weaknesses': ['Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù', 'Ù…Ø´Ú©Ù„Ø§Øª', 'Ø¶Ø¹Ùâ€ŒÙ‡Ø§'],
            'recommendations': ['ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§', 'Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª', 'Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§'],
            'improvement': ['Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯', 'Ù…Ø±Ø§Ø­Ù„ Ø§Ø¬Ø±Ø§', 'Ø¨Ù‡Ø¨ÙˆØ¯']
        }
        
        for section_name, patterns in section_patterns.items():
            for pattern in patterns:
                if pattern in analysis_text:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§ÛŒÙ† Ø¨Ø®Ø´
                    start_idx = analysis_text.find(pattern)
                    if start_idx != -1:
                        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù¾Ø§ÛŒØ§Ù† Ø¨Ø®Ø´
                        end_idx = start_idx + 500  # Ø­Ø¯Ø§Ú©Ø«Ø± 500 Ú©Ø§Ø±Ø§Ú©ØªØ±
                        sections[section_name] = analysis_text[start_idx:end_idx]
                        break
        
        return sections
    
    def _extract_recommendations(self, analysis_text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ Ø§Ø² Ù…ØªÙ† ØªØ­Ù„ÛŒÙ„"""
        recommendations = []
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        import re
        numbered_items = re.findall(r'\d+\.\s*([^\n]+)', analysis_text)
        recommendations.extend(numbered_items[:5])  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 Ù…ÙˆØ±Ø¯
        
        return recommendations
    
    def _extract_strengths(self, analysis_text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· Ù‚ÙˆØª"""
        strengths = []
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        strength_keywords = ['Ù‚ÙˆØª', 'Ù…Ø²ÛŒØª', 'Ø®ÙˆØ¨', 'Ù…Ù†Ø§Ø³Ø¨', 'Ø¹Ø§Ù„ÛŒ']
        
        sentences = analysis_text.split('.')
        for sentence in sentences:
            for keyword in strength_keywords:
                if keyword in sentence and len(sentence.strip()) > 10:
                    strengths.append(sentence.strip())
                    break
        
        return strengths[:3]  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ù…ÙˆØ±Ø¯
    
    def _extract_weaknesses(self, analysis_text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù"""
        weaknesses = []
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        weakness_keywords = ['Ø¶Ø¹Ù', 'Ù…Ø´Ú©Ù„', 'Ù†ÛŒØ§Ø²', 'Ø¨Ù‡Ø¨ÙˆØ¯', 'Ú©Ù…Ø¨ÙˆØ¯']
        
        sentences = analysis_text.split('.')
        for sentence in sentences:
            for keyword in weakness_keywords:
                if keyword in sentence and len(sentence.strip()) > 10:
                    weaknesses.append(sentence.strip())
                    break
        
        return weaknesses[:3]  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ù…ÙˆØ±Ø¯
    
    def _extract_improvement_plan(self, analysis_text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯"""
        plan = []
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø±Ø§Ø­Ù„
        import re
        steps = re.findall(r'(Ù…Ø±Ø­Ù„Ù‡|Ú¯Ø§Ù…|Ù‚Ø¯Ù…)\s*\d*[:\-]?\s*([^\n]+)', analysis_text)
        for step in steps:
            plan.append(step[1].strip())
        
        return plan[:5]  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 Ù…Ø±Ø­Ù„Ù‡
    
    def _get_default_analysis_result(self, store_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ù†ØªÛŒØ¬Ù‡ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§"""
        return {
            'overall_score': 6.0,
            'analysis_text': self._get_fallback_analysis(),
            'sections': {
                'overall': 'ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡',
                'strengths': 'Ù†Ù‚Ø§Ø· Ù‚ÙˆØª ÙØ±ÙˆØ´Ú¯Ø§Ù‡',
                'weaknesses': 'Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù ÙØ±ÙˆØ´Ú¯Ø§Ù‡',
                'recommendations': 'ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯'
            },
            'recommendations': [
                'Ø¨Ù‡Ø¨ÙˆØ¯ Ú†ÛŒØ¯Ù…Ø§Ù† Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§',
                'Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ',
                'Ø§ÙØ²Ø§ÛŒØ´ Ú©Ø§Ø±Ø§ÛŒÛŒ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§',
                'Ø¨Ù‡Ø¨ÙˆØ¯ Ø¬Ø±ÛŒØ§Ù† Ù…Ø´ØªØ±ÛŒ',
                'Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª'
            ],
            'strengths': [
                'Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ Ù…Ù†Ø§Ø³Ø¨',
                'Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø±Ø´Ø¯ Ø®ÙˆØ¨',
                'Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ Ù…Ù†Ø§Ø³Ø¨'
            ],
            'weaknesses': [
                'Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ú†ÛŒØ¯Ù…Ø§Ù†',
                'Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ',
                'Ø§ÙØ²Ø§ÛŒØ´ Ú©Ø§Ø±Ø§ÛŒÛŒ'
            ],
            'improvement_plan': [
                'Ù…Ø±Ø­Ù„Ù‡ 1: ØªØ­Ù„ÛŒÙ„ ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ',
                'Ù…Ø±Ø­Ù„Ù‡ 2: Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯',
                'Ù…Ø±Ø­Ù„Ù‡ 3: Ø§Ø¬Ø±Ø§ÛŒ ØªØºÛŒÛŒØ±Ø§Øª',
                'Ù…Ø±Ø­Ù„Ù‡ 4: Ù†Ø¸Ø§Ø±Øª Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ'
            ],
            'created_at': datetime.now().isoformat()
        }
    
    def _initialize_ml_models(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ML"""
        try:
            if SKLEARN_AVAILABLE:
                from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
                
                # Sales prediction model
                self.ml_models['sales_predictor'] = RandomForestRegressor(
                    n_estimators=100,
                    random_state=42
                )
                
                # Conversion rate predictor
                self.ml_models['conversion_predictor'] = RandomForestRegressor(
                    n_estimators=100,
                    random_state=42
                )
                
                # Customer behavior classifier
                self.ml_models['behavior_classifier'] = RandomForestClassifier(
                    n_estimators=100,
                    random_state=42
                )
            
            # Neural network for complex patterns
            self.ml_models['neural_network'] = self._create_neural_network()
            
            logger.info("ML models initialized successfully")
            
        except Exception as e:
            logger.error(f"Error initializing ML models: {e}")
            global ML_AVAILABLE
            ML_AVAILABLE = False
    
    def _create_neural_network(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡"""
        if not TENSORFLOW_AVAILABLE:
            return None
            
        try:
            from tensorflow import keras
            
            model = keras.Sequential([
                keras.layers.Dense(64, activation='relu', input_shape=(20,)),
                keras.layers.Dropout(0.2),
                keras.layers.Dense(32, activation='relu'),
                keras.layers.Dropout(0.2),
                keras.layers.Dense(16, activation='relu'),
                keras.layers.Dense(1, activation='linear')
            ])
            
            model.compile(
                optimizer='adam',
                loss='mse',
                metrics=['mae']
            )
            
            return model
            
        except Exception as e:
            logger.error(f"Error creating neural network: {e}")
            return None
    
    def generate_detailed_analysis(self, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ ØªÙØµÛŒÙ„ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AI"""
        try:
            # Ø§Ú¯Ø± OpenAI Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² ØªØ­Ù„ÛŒÙ„ Ù…Ø­Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            if not self.openai_client:
                return self._generate_local_analysis(analysis_data)
            
            # ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ OpenAI
            return self._generate_openai_analysis(analysis_data)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ AI: {e}")
            return self._generate_local_analysis(analysis_data)
    
    def generate_advanced_ml_analysis(self, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ML"""
        if not ML_AVAILABLE:
            return {"error": "ML libraries not available"}
        
        try:
            # ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ML
            features = self._extract_ml_features(analysis_data)
            
            # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
            predictions = {
                "sales_prediction": self._predict_sales(features),
                "conversion_optimization": self._predict_conversion_improvement(features),
                "customer_behavior": self._analyze_customer_behavior(features),
                "optimization_priority": self._get_optimization_priority(features),
                "roi_prediction": self._predict_roi(features)
            }
            
            # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§
            pattern_analysis = self._analyze_patterns(features)
            
            # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± ML
            ml_recommendations = self._generate_ml_recommendations(features, predictions)
            
            return {
                "ml_predictions": predictions,
                "pattern_analysis": pattern_analysis,
                "ml_recommendations": ml_recommendations,
                "confidence_scores": self._calculate_confidence_scores(features),
                "generated_at": datetime.now().isoformat(),
                "analysis_type": "advanced_ml"
            }
            
        except Exception as e:
            logger.error(f"Error in ML analysis: {e}")
            return {"error": f"ML analysis failed: {str(e)}"}
    
    def _extract_ml_features(self, analysis_data: Dict[str, Any]):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ML Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        features = []
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯
        def safe_float(value, default=0.0):
            try:
                if isinstance(value, str):
                    return float(value)
                return float(value) if value is not None else default
            except (ValueError, TypeError):
                return default
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        features.extend([
            safe_float(analysis_data.get('store_size', 500)),
            safe_float(analysis_data.get('entrance_count', 2)),
            safe_float(analysis_data.get('checkout_count', 3)),
            safe_float(analysis_data.get('shelf_count', 25)),
            safe_float(analysis_data.get('conversion_rate', 35.5)),
            safe_float(analysis_data.get('customer_traffic', 150)),
            safe_float(analysis_data.get('customer_dwell_time', 45)),
            safe_float(analysis_data.get('unused_area_size', 0)),
            safe_float(analysis_data.get('daily_sales_volume', 0)),
            safe_float(analysis_data.get('supplier_count', 0)),
            safe_float(analysis_data.get('camera_count', 0)),
            safe_float(analysis_data.get('morning_sales_percent', 30)),
            safe_float(analysis_data.get('noon_sales_percent', 40)),
            safe_float(analysis_data.get('evening_sales_percent', 30)),
            safe_float(analysis_data.get('sales_improvement_target', 20)),
            safe_float(analysis_data.get('optimization_timeline', 6)),
            safe_float(analysis_data.get('historical_data_months', 12)),
        ])
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù„ÛŒØ³ØªÛŒ Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡
        product_categories = analysis_data.get('product_categories', [])
        if isinstance(product_categories, list):
            features.append(len(product_categories))
        elif isinstance(product_categories, str):
            features.append(1)  # Ø§Ú¯Ø± Ø±Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ ÛŒÚ© Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
        else:
            features.append(0)
        
        peak_days = analysis_data.get('peak_days', [])
        if isinstance(peak_days, list):
            features.append(len(peak_days))
        elif isinstance(peak_days, str):
            features.append(1)  # Ø§Ú¯Ø± Ø±Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ ÛŒÚ© Ø±ÙˆØ² Ø§ÙˆØ¬ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
        else:
            features.append(0)
        
        return np.array(features).reshape(1, -1)
    
    def _predict_sales(self, features) -> Dict[str, Any]:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÙØ±ÙˆØ´ Ø¨Ø§ ML"""
        try:
            # Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ø¨Ø§Ø´Ø¯
            # Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ØŒ Ø§Ø² ÛŒÚ© Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            store_size = float(features[0, 0])
            conversion_rate = float(features[0, 4])
            customer_traffic = float(features[0, 5])
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ±ÙˆØ´ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡
            predicted_sales = customer_traffic * (conversion_rate / 100) * 1000  # Ù…ØªÙˆØ³Ø· Ø®Ø±ÛŒØ¯ 1000 ØªÙˆÙ…Ø§Ù†
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø¨Ù‡Ø¨ÙˆØ¯
            improvement_potential = min(50, (50 - conversion_rate) * 2)
            
            return {
                "current_daily_sales": predicted_sales,
                "potential_daily_sales": predicted_sales * (1 + improvement_potential / 100),
                "improvement_potential": improvement_potential,
                "confidence": 0.85
            }
            
        except Exception as e:
            logger.error(f"Error in sales prediction: {e}")
            return {"error": str(e)}
    
    def _predict_conversion_improvement(self, features) -> Dict[str, Any]:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„"""
        try:
            current_conversion = float(features[0, 4])
            store_size = float(features[0, 0])
            unused_area = float(features[0, 7])
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø¨Ù‡Ø¨ÙˆØ¯
            layout_improvement = min(15, (store_size - unused_area) / store_size * 20)
            checkout_improvement = min(10, float(features[0, 2]) * 2)  # Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§
            lighting_improvement = 5  # Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ
            
            total_improvement = layout_improvement + checkout_improvement + lighting_improvement
            
            return {
                "current_conversion_rate": current_conversion,
                "predicted_improvement": total_improvement,
                "new_conversion_rate": min(100, current_conversion + total_improvement),
                "factors": {
                    "layout": layout_improvement,
                    "checkout": checkout_improvement,
                    "lighting": lighting_improvement
                },
                "confidence": 0.80
            }
            
        except Exception as e:
            logger.error(f"Error in conversion prediction: {e}")
            return {"error": str(e)}
    
    def _analyze_customer_behavior(self, features) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒ"""
        try:
            dwell_time = float(features[0, 6])
            traffic = float(features[0, 5])
            conversion = float(features[0, 4])
            
            # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø±ÙØªØ§Ø±
            if dwell_time > 60 and conversion > 40:
                behavior_type = "high_engagement"
                description = "Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø§ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§Ù„Ø§ Ùˆ Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„ Ø®ÙˆØ¨"
            elif dwell_time > 30 and conversion > 30:
                behavior_type = "moderate_engagement"
                description = "Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø§ ØªØ¹Ø§Ù…Ù„ Ù…ØªÙˆØ³Ø·"
            else:
                behavior_type = "low_engagement"
                description = "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ØªØ¹Ø§Ù…Ù„ Ù…Ø´ØªØ±ÛŒ"
            
            return {
                "behavior_type": behavior_type,
                "description": description,
                "engagement_score": min(100, (dwell_time / 60) * 50 + (conversion / 50) * 50),
                "recommendations": self._get_behavior_recommendations(behavior_type),
                "practical_guide": self._generate_practical_recommendations(features)
            }
            
        except Exception as e:
            logger.error(f"Error in behavior analysis: {e}")
            return {"error": str(e)}
    
    def _get_behavior_recommendations(self, behavior_type: str) -> List[str]:
        """Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø±ÙØªØ§Ø±"""
        recommendations = {
            "high_engagement": [
                "Ø­ÙØ¸ Ú©ÛŒÙÛŒØª Ø®Ø¯Ù…Ø§Øª",
                "Ø§ÙØ²Ø§ÛŒØ´ ØªÙ†ÙˆØ¹ Ù…Ø­ØµÙˆÙ„Ø§Øª",
                "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆÙØ§Ø¯Ø§Ø±ÛŒ"
            ],
            "moderate_engagement": [
                "Ø¨Ù‡Ø¨ÙˆØ¯ Ú†ÛŒØ¯Ù…Ø§Ù†",
                "Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø§Ù…Ù„",
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§"
            ],
            "low_engagement": [
                "Ø¨Ø§Ø²Ø·Ø±Ø§Ø­ÛŒ Ú©Ø§Ù…Ù„",
                "Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ø±Ú©Ù†Ø§Ù†",
                "Ø¨Ù‡Ø¨ÙˆØ¯ ØªØ¬Ø±Ø¨Ù‡ Ù…Ø´ØªØ±ÛŒ"
            ]
        }
        return recommendations.get(behavior_type, [])
    
    def _generate_practical_recommendations(self, features) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù†"""
        try:
            store_size = float(features[0, 0])
            entrance_count = float(features[0, 1])
            checkout_count = float(features[0, 2])
            shelf_count = float(features[0, 3])
            conversion_rate = float(features[0, 4])
            customer_traffic = float(features[0, 5])
            unused_area = float(features[0, 7])
            
            practical_guide = {
                "window_display": self._get_window_display_guide(store_size, conversion_rate),
                "shelf_layout": self._get_shelf_layout_guide(shelf_count, store_size),
                "path_design": self._get_path_design_guide(store_size, customer_traffic),
                "lighting": self._get_lighting_guide(store_size, conversion_rate),
                "color_scheme": self._get_color_scheme_guide(conversion_rate),
                "product_placement": self._get_product_placement_guide(shelf_count, conversion_rate)
            }
            
            return practical_guide
            
        except Exception as e:
            logger.error(f"Error generating practical recommendations: {e}")
            return {"error": str(e)}
    
    def _get_window_display_guide(self, store_size: float, conversion_rate: float) -> Dict[str, Any]:
        """Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø·Ø±Ø§Ø­ÛŒ ÙˆÛŒØªØ±ÛŒÙ†"""
        height = "1.2-1.8 Ù…ØªØ±" if store_size > 200 else "1.0-1.6 Ù…ØªØ±"
        lighting = "LED 3000K" if conversion_rate < 40 else "LED 4000K"
        
        return {
            "height": height,
            "lighting": lighting,
            "color_rule": "Ù‚Ø§Ù†ÙˆÙ† 60-30-10",
            "product_count": "3-5 Ù…Ø­ØµÙˆÙ„ Ø§ØµÙ„ÛŒ",
            "rotation_frequency": "Ù‡ÙØªÚ¯ÛŒ",
            "tips": [
                "Ù…Ø­ØµÙˆÙ„Ø§Øª Ù¾Ø±ÙØ±ÙˆØ´ Ø¯Ø± Ù…Ø±Ú©Ø² ÙˆÛŒØªØ±ÛŒÙ†",
                "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ø³Ø§Ø¯Ù‡",
                "Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ ÛŒÚ©Ù†ÙˆØ§Ø®Øª",
                "ØªØºÛŒÛŒØ± Ù…Ù†Ø¸Ù… Ù…Ø­ØªÙˆØ§"
            ]
        }
    
    def _get_shelf_layout_guide(self, shelf_count: float, store_size: float) -> Dict[str, Any]:
        """Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§"""
        shelf_height = "0.3-2.1 Ù…ØªØ±"
        aisle_width = "1.2-1.8 Ù…ØªØ±" if store_size > 300 else "1.0-1.5 Ù…ØªØ±"
        
        return {
            "shelf_heights": {
                "bottom": "0.3-0.6 Ù…ØªØ±",
                "middle": "0.6-1.5 Ù…ØªØ±",
                "top": "1.5-2.1 Ù…ØªØ±"
            },
            "aisle_width": aisle_width,
            "product_arrangement": "Ù…Ø­ØµÙˆÙ„Ø§Øª Ù¾Ø±ÙØ±ÙˆØ´ Ø¯Ø± Ø³Ø·Ø­ Ú†Ø´Ù… (1.5 Ù…ØªØ±)",
            "spacing": "ÙØ§ØµÙ„Ù‡ 15-20 Ø³Ø§Ù†ØªÛŒâ€ŒÙ…ØªØ± Ø¨ÛŒÙ† Ù…Ø­ØµÙˆÙ„Ø§Øª",
            "tips": [
                "Ø§Ø¬ØªÙ†Ø§Ø¨ Ø§Ø² Ø¨Ù†â€ŒØ¨Ø³Øª",
                "Ø¯Ø³ØªØ±Ø³ÛŒ Ø¢Ø³Ø§Ù† Ø¨Ù‡ Ù‡Ù…Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª",
                "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ù„ÙˆÙ‡Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§",
                "Ù†Ø¸Ù… Ùˆ ØªØ±ØªÛŒØ¨ Ù…Ù†Ø¸Ù…"
            ]
        }
    
    def _get_path_design_guide(self, store_size: float, customer_traffic: float) -> Dict[str, Any]:
        """Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø·Ø±Ø§Ø­ÛŒ Ù…Ø³ÛŒØ±"""
        main_path_width = "2.4-3.0 Ù…ØªØ±" if customer_traffic > 100 else "2.0-2.4 Ù…ØªØ±"
        secondary_path_width = "1.8-2.4 Ù…ØªØ±"
        
        return {
            "main_path": {
                "width": main_path_width,
                "direction": "Ø§Ø² ÙˆØ±ÙˆØ¯ÛŒ ØªØ§ ØµÙ†Ø¯ÙˆÙ‚",
                "products": "Ù…Ø­ØµÙˆÙ„Ø§Øª Ù¾Ø±ÙØ±ÙˆØ´ Ø¯Ø± Ú©Ù†Ø§Ø± Ù…Ø³ÛŒØ±"
            },
            "secondary_paths": {
                "width": secondary_path_width,
                "purpose": "Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"
            },
            "stopping_points": {
                "size": "1.5Ã—1.5 Ù…ØªØ±",
                "purpose": "Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª"
            },
            "tips": [
                "Ù…Ø³ÛŒØ± Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ø¨Ø¯ÙˆÙ† Ù…Ø§Ù†Ø¹",
                "Ù†Ù‚Ø§Ø· ØªÙˆÙ‚Ù Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒÚ©",
                "Ø¯Ø³ØªØ±Ø³ÛŒ Ø¢Ø³Ø§Ù† Ø¨Ù‡ ØµÙ†Ø¯ÙˆÙ‚",
                "ÙØ¶Ø§ÛŒ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø¨Ø¯ Ø®Ø±ÛŒØ¯"
            ]
        }
    
    def _get_lighting_guide(self, store_size: float, conversion_rate: float) -> Dict[str, Any]:
        """Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ"""
        intensity = "500-800 Ù„ÙˆÚ©Ø³" if conversion_rate < 40 else "300-500 Ù„ÙˆÚ©Ø³"
        color_temp = "3000K (Ú¯Ø±Ù…)" if store_size < 200 else "4000K (Ø®Ù†Ø«ÛŒ)"
        
        return {
            "general_lighting": {
                "intensity": intensity,
                "color_temperature": color_temp,
                "type": "LED"
            },
            "accent_lighting": {
                "purpose": "ØªØ£Ú©ÛŒØ¯ Ø¨Ø± Ù…Ø­ØµÙˆÙ„Ø§Øª Ø®Ø§Øµ",
                "intensity": "800-1200 Ù„ÙˆÚ©Ø³",
                "color_temperature": "2700K"
            },
            "tips": [
                "Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ ÛŒÚ©Ù†ÙˆØ§Ø®Øª",
                "Ø§Ø¬ØªÙ†Ø§Ø¨ Ø§Ø² Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ ØªÛŒØ²",
                "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†ÙˆØ± Ø·Ø¨ÛŒØ¹ÛŒ",
                "Ú©Ù†ØªØ±Ù„ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø¹Øª"
            ]
        }
    
    def _get_color_scheme_guide(self, conversion_rate: float) -> Dict[str, Any]:
        """Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ Ø±Ù†Ú¯ÛŒ"""
        if conversion_rate < 30:
            scheme = "Ú¯Ø±Ù… Ùˆ Ø§Ù†Ø±Ú˜ÛŒâ€ŒØ¨Ø®Ø´"
            colors = ["Ù‚Ø±Ù…Ø²", "Ù†Ø§Ø±Ù†Ø¬ÛŒ", "Ø²Ø±Ø¯"]
        elif conversion_rate < 45:
            scheme = "Ù…ØªØ¹Ø§Ø¯Ù„ Ùˆ Ù…ØªÙˆØ§Ø²Ù†"
            colors = ["Ø¢Ø¨ÛŒ", "Ø³Ø¨Ø²", "Ø®Ø§Ú©Ø³ØªØ±ÛŒ"]
        else:
            scheme = "Ù„ÙˆÚ©Ø³ Ùˆ Ø¢Ø±Ø§Ù…"
            colors = ["Ø¨Ù†ÙØ´", "Ø³ÙÛŒØ¯", "Ø³ÛŒØ§Ù‡"]
        
        return {
            "scheme": scheme,
            "primary_colors": colors,
            "rule": "Ù‚Ø§Ù†ÙˆÙ† 60-30-10",
            "usage": {
                "60%": "Ø±Ù†Ú¯ Ø§ØµÙ„ÛŒ (Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡)",
                "30%": "Ø±Ù†Ú¯ Ø«Ø§Ù†ÙˆÛŒÙ‡ (Ù‚Ø§Ø¨â€ŒØ¨Ù†Ø¯ÛŒ)",
                "10%": "Ø±Ù†Ú¯ ØªØ£Ú©ÛŒØ¯ÛŒ (Ø¬Ø²Ø¦ÛŒØ§Øª)"
            }
        }
    
    def _get_product_placement_guide(self, shelf_count: float, conversion_rate: float) -> Dict[str, Any]:
        """Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù‚Ø±Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª"""
        return {
            "high_traffic_areas": [
                "Ú©Ù†Ø§Ø± ÙˆØ±ÙˆØ¯ÛŒ",
                "Ù†Ø²Ø¯ÛŒÚ© ØµÙ†Ø¯ÙˆÙ‚",
                "Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ"
            ],
            "product_arrangement": {
                "eye_level": "Ù…Ø­ØµÙˆÙ„Ø§Øª Ù¾Ø±ÙØ±ÙˆØ´",
                "top_shelf": "Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¬Ø¯ÛŒØ¯",
                "bottom_shelf": "Ù…Ø­ØµÙˆÙ„Ø§Øª Ø­Ø¬ÛŒÙ…"
            },
            "cross_selling": {
                "strategy": "Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…Ø±ØªØ¨Ø· Ø¯Ø± Ú©Ù†Ø§Ø± Ù‡Ù…",
                "examples": [
                    "Ú©ÙØ´ Ùˆ Ø¬ÙˆØ±Ø§Ø¨",
                    "Ù„Ø¨Ø§Ø³ Ùˆ Ø§Ú©Ø³Ø³ÙˆØ±ÛŒ",
                    "Ù…ÙˆØ§Ø¯ ØºØ°Ø§ÛŒÛŒ Ùˆ Ù†ÙˆØ´ÛŒØ¯Ù†ÛŒ"
                ]
            },
            "seasonal_placement": {
                "front": "Ù…Ø­ØµÙˆÙ„Ø§Øª ÙØµÙ„ÛŒ",
                "back": "Ù…Ø­ØµÙˆÙ„Ø§Øª Ù‡Ù…ÛŒØ´Ú¯ÛŒ"
            }
        }
    
    def _get_optimization_priority(self, features) -> Dict[str, Any]:
        """Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        try:
            priorities = []
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ø®Ø´
            layout_score = 100 - (float(features[0, 7]) / float(features[0, 0]) * 100)  # ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡
            checkout_score = float(features[0, 2]) * 10  # ØªØ¹Ø¯Ø§Ø¯ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§
            conversion_score = float(features[0, 4])  # Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„
            traffic_score = float(features[0, 5]) / 10  # ØªØ±Ø§ÙÛŒÚ©
            
            # Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ
            if layout_score < 70:
                priorities.append({"area": "layout", "priority": "high", "score": layout_score})
            if checkout_score < 30:
                priorities.append({"area": "checkout", "priority": "high", "score": checkout_score})
            if conversion_score < 35:
                priorities.append({"area": "conversion", "priority": "medium", "score": conversion_score})
            if traffic_score < 10:
                priorities.append({"area": "traffic", "priority": "low", "score": traffic_score})
            
            return {
                "priorities": sorted(priorities, key=lambda x: x["score"]),
                "overall_score": (layout_score + checkout_score + conversion_score + traffic_score) / 4
            }
            
        except Exception as e:
            logger.error(f"Error in optimization priority: {e}")
            return {"error": str(e)}
    
    def _predict_roi(self, features) -> Dict[str, Any]:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§Ø²Ú¯Ø´Øª Ø³Ø±Ù…Ø§ÛŒÙ‡"""
        try:
            current_sales = float(features[0, 9])  # ÙØ±ÙˆØ´ Ø±ÙˆØ²Ø§Ù†Ù‡
            improvement_target = float(features[0, 15])  # Ù‡Ø¯Ù Ø¨Ù‡Ø¨ÙˆØ¯
            timeline = float(features[0, 16])  # Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ROI
            additional_sales = current_sales * (improvement_target / 100) * 365  # ÙØ±ÙˆØ´ Ø³Ø§Ù„Ø§Ù†Ù‡ Ø§Ø¶Ø§ÙÛŒ
            estimated_cost = current_sales * 0.1 * timeline  # Ù‡Ø²ÛŒÙ†Ù‡ ØªØ®Ù…ÛŒÙ†ÛŒ (10% ÙØ±ÙˆØ´ ÙØ¹Ù„ÛŒ)
            roi = (additional_sales - estimated_cost) / estimated_cost * 100
            
            return {
                "current_annual_sales": current_sales * 365,
                "additional_annual_sales": additional_sales,
                "estimated_cost": estimated_cost,
                "roi_percentage": roi,
                "payback_period": timeline,
                "confidence": 0.75
            }
            
        except Exception as e:
            logger.error(f"Error in ROI prediction: {e}")
            return {"error": str(e)}
    
    def _analyze_patterns(self, features) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§"""
        try:
            patterns = {
                "traffic_patterns": self._analyze_traffic_patterns(features),
                "sales_patterns": self._analyze_sales_patterns(features),
                "seasonal_patterns": self._analyze_seasonal_patterns(features)
            }
            return patterns
            
        except Exception as e:
            logger.error(f"Error in pattern analysis: {e}")
            return {"error": str(e)}
    
    def _analyze_traffic_patterns(self, features) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ±Ø§ÙÛŒÚ©"""
        morning = float(features[0, 11])
        noon = float(features[0, 12])
        evening = float(features[0, 13])
        
        peak_period = "morning" if morning > max(noon, evening) else "noon" if noon > evening else "evening"
        
        return {
            "peak_period": peak_period,
            "distribution": {
                "morning": morning,
                "noon": noon,
                "evening": evening
            },
            "recommendations": self._get_traffic_recommendations(peak_period)
        }
    
    def _get_traffic_recommendations(self, peak_period: str) -> List[str]:
        """Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯ÙˆØ±Ù‡ Ù¾ÛŒÚ©"""
        recommendations = {
            "morning": [
                "Ø§ÙØ²Ø§ÛŒØ´ Ú©Ø§Ø±Ú©Ù†Ø§Ù† Ø¯Ø± Ø³Ø§Ø¹Ø§Øª ØµØ¨Ø­",
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¹Ø§Øª ØµØ¨Ø­",
                "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ´ÙˆÛŒÙ‚ÛŒ ØµØ¨Ø­Ú¯Ø§Ù‡ÛŒ"
            ],
            "noon": [
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¹Ø§Øª Ø´Ù„ÙˆØºÛŒ",
                "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‡Ø§Ø±",
                "Ù…Ø¯ÛŒØ±ÛŒØª ØµÙ Ù‡ÙˆØ´Ù…Ù†Ø¯"
            ],
            "evening": [
                "Ø§ÙØ²Ø§ÛŒØ´ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ",
                "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ±Ú¯Ø§Ù‡ÛŒ",
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬"
            ]
        }
        return recommendations.get(peak_period, [])
    
    def _analyze_sales_patterns(self, features) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙØ±ÙˆØ´"""
        conversion_rate = float(features[0, 4])
        customer_traffic = float(features[0, 5])
        
        efficiency_score = (conversion_rate / 50) * (customer_traffic / 200) * 100
        
        return {
            "efficiency_score": efficiency_score,
            "performance_level": "high" if efficiency_score > 70 else "medium" if efficiency_score > 40 else "low",
            "optimization_potential": 100 - efficiency_score
        }
    
    def _analyze_seasonal_patterns(self, features) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙØµÙ„ÛŒ"""
        # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¯Ø§Ø±Ø¯
        return {
            "note": "ØªØ­Ù„ÛŒÙ„ ÙØµÙ„ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¯Ø§Ø±Ø¯",
            "recommendation": "Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ Ù…Ø§Ù‡Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙØµÙ„ÛŒ"
        }
    
    def _generate_ml_recommendations(self, features, predictions: Dict) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± ML"""
        try:
            recommendations = {
                "immediate": [],
                "short_term": [],
                "long_term": []
            }
            
            # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª ÙÙˆØ±ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„
            if predictions.get("conversion_optimization", {}).get("predicted_improvement", 0) > 10:
                recommendations["immediate"].append("Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙÙˆØ±ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„")
            
            if float(features[0, 7]) > float(features[0, 0]) * 0.2:  # ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡ > 20%
                recommendations["immediate"].append("Ø¨Ø§Ø²Ø·Ø±Ø§Ø­ÛŒ ÙÙˆØ±ÛŒ ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡")
            
            # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ú©ÙˆØªØ§Ù‡ Ù…Ø¯Øª
            if predictions.get("roi_prediction", {}).get("roi_percentage", 0) > 50:
                recommendations["short_term"].append("Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø§ ROI Ø¨Ø§Ù„Ø§")
            
            # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù„Ù†Ø¯ Ù…Ø¯Øª
            if predictions.get("sales_prediction", {}).get("improvement_potential", 0) > 30:
                recommendations["long_term"].append("Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø¨Ø±Ø§ÛŒ Ø­Ø¯Ø§Ú©Ø«Ø± Ù¾ØªØ§Ù†Ø³ÛŒÙ„")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error generating ML recommendations: {e}")
            return {"error": str(e)}
    
    def _calculate_confidence_scores(self, features) -> Dict[str, float]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ø§Ø·Ù…ÛŒÙ†Ø§Ù†"""
        try:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            data_completeness = min(100, np.count_nonzero(features) / len(features) * 100)
            data_consistency = 85  # ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø³Ø§Ø²Ú¯Ø§Ø± Ù‡Ø³ØªÙ†Ø¯
            
            return {
                "data_completeness": data_completeness,
                "data_consistency": data_consistency,
                "overall_confidence": (data_completeness + data_consistency) / 2
            }
            
        except Exception as e:
            logger.error(f"Error calculating confidence scores: {e}")
            return {"error": str(e)}
    
    def _generate_openai_analysis(self, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ OpenAI"""
        try:
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ AI
            store_info = self._prepare_store_info(analysis_data)
            
            # Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
            prompt = f"""
            Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ ØªØ­Ù„ÛŒÙ„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØ¬Ø§Ø±Øª Ù‡Ø³ØªÛŒØ¯. 
            Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ ØªÙØµÛŒÙ„ÛŒ Ùˆ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø²ÛŒØ± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯:

            Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ±ÙˆØ´Ú¯Ø§Ù‡:
            {store_info}

            Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ù‚Ø§Ù„Ø¨ JSON Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø²ÛŒØ± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯:
            {{
                "executive_summary": "Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ",
                "detailed_analysis": {{
                    "strengths": ["Ù†Ù‚Ø§Ø· Ù‚ÙˆØª"],
                    "weaknesses": ["Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù"],
                    "opportunities": ["ÙØ±ØµØªâ€ŒÙ‡Ø§"],
                    "threats": ["ØªÙ‡Ø¯ÛŒØ¯Ù‡Ø§"]
                }},
                "recommendations": {{
                    "immediate": ["Ø§Ù‚Ø¯Ø§Ù…Ø§Øª ÙÙˆØ±ÛŒ"],
                    "short_term": ["Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ú©ÙˆØªØ§Ù‡ Ù…Ø¯Øª"],
                    "long_term": ["Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ø¨Ù„Ù†Ø¯ Ù…Ø¯Øª"]
                }},
                "optimization_plan": {{
                    "layout_optimization": "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù†",
                    "pricing_strategy": "Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù‚ÛŒÙ…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ",
                    "inventory_management": "Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÙˆØ¬ÙˆØ¯ÛŒ",
                    "customer_experience": "ØªØ¬Ø±Ø¨Ù‡ Ù…Ø´ØªØ±ÛŒ"
                }},
                "financial_projections": {{
                    "sales_increase": "Ø¯Ø±ØµØ¯ Ø§ÙØ²Ø§ÛŒØ´ ÙØ±ÙˆØ´",
                    "cost_reduction": "Ø¯Ø±ØµØ¯ Ú©Ø§Ù‡Ø´ Ù‡Ø²ÛŒÙ†Ù‡",
                    "roi_timeline": "Ø²Ù…Ø§Ù† Ø¨Ø§Ø²Ú¯Ø´Øª Ø³Ø±Ù…Ø§ÛŒÙ‡"
                }},
                "implementation_timeline": {{
                    "phase_1": "ÙØ§Ø² Ø§ÙˆÙ„ (1-2 Ù…Ø§Ù‡)",
                    "phase_2": "ÙØ§Ø² Ø¯ÙˆÙ… (3-6 Ù…Ø§Ù‡)",
                    "phase_3": "ÙØ§Ø² Ø³ÙˆÙ… (6-12 Ù…Ø§Ù‡)"
                }}
            }}
            """
            
            # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ OpenAI
            response = self.openai_client.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ ØªØ­Ù„ÛŒÙ„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØ¬Ø§Ø±Øª Ù‡Ø³ØªÛŒØ¯."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø§Ø³Ø®
            ai_response = response.choices[0].message.content
            analysis_result = json.loads(ai_response)
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ØªØ§Ø¯ÛŒØªØ§
            analysis_result['generated_at'] = datetime.now().isoformat()
            analysis_result['ai_model'] = 'gpt-3.5-turbo'
            analysis_result['confidence_score'] = 0.95
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ OpenAI: {e}")
            return self._generate_local_analysis(analysis_data)
    
    def _generate_local_analysis(self, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Ù…Ø­Ù„ÛŒ (Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ API)"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
        store_size = analysis_data.get('store_size', 500)
        entrance_count = analysis_data.get('entrance_count', 2)
        checkout_count = analysis_data.get('checkout_count', 3)
        conversion_rate = analysis_data.get('conversion_rate', 35.5)
        customer_traffic = analysis_data.get('customer_traffic', 150)
        customer_dwell_time = analysis_data.get('customer_dwell_time', 45)
        unused_area_size = analysis_data.get('unused_area_size', 0)
        daily_sales_volume = analysis_data.get('daily_sales_volume', 0)
        product_categories = analysis_data.get('product_categories', [])
        has_surveillance = analysis_data.get('has_surveillance', False)
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡
        uploaded_files_count = sum([
            1 if analysis_data.get('store_photos') else 0,
            1 if analysis_data.get('store_plan') else 0,
            1 if analysis_data.get('shelf_photos') else 0,
            1 if analysis_data.get('entrance_photos') else 0,
            1 if analysis_data.get('checkout_photos') else 0,
            1 if analysis_data.get('customer_video') else 0,
            1 if analysis_data.get('surveillance_footage') else 0,
            1 if analysis_data.get('sales_file') else 0,
            1 if analysis_data.get('product_catalog') else 0,
        ])
        
        # ØªØ­Ù„ÛŒÙ„ Ù†Ù‚Ø§Ø· Ù‚ÙˆØª
        strengths = []
        if entrance_count >= 2:
            strengths.append("Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø±ÛŒ Ø§Ø² ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§ÙÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Ù‡ÛŒÙ„ Ø¬Ø±ÛŒØ§Ù† Ù…Ø´ØªØ±ÛŒØ§Ù†")
        if checkout_count >= 3:
            strengths.append("Ø¸Ø±ÙÛŒØª Ù…Ù†Ø§Ø³Ø¨ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª")
        if conversion_rate > 30:
            strengths.append("Ø¯Ø§Ø´ØªÙ† Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„")
        if customer_traffic > 100:
            strengths.append("Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø±ÛŒ Ø§Ø² ØªØ±Ø§ÙÛŒÚ© Ù…Ø´ØªØ±ÛŒØ§Ù† Ø±ÙˆØ²Ø§Ù†Ù‡ Ù…Ø·Ù„ÙˆØ¨")
        if customer_dwell_time > 30:
            strengths.append("Ø²Ù…Ø§Ù† Ø­Ø¶ÙˆØ± Ù…Ù†Ø§Ø³Ø¨ Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¯Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡")
        if has_surveillance:
            strengths.append("ÙˆØ¬ÙˆØ¯ Ø³ÛŒØ³ØªÙ… Ù†Ø¸Ø§Ø±Øª Ùˆ Ø§Ù…Ù†ÛŒØª")
        if len(product_categories) > 3:
            strengths.append("ØªÙ†ÙˆØ¹ Ù…Ù†Ø§Ø³Ø¨ Ø¯Ø± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª")
        if uploaded_files_count > 5:
            strengths.append("Ø§Ø±Ø§Ø¦Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ùˆ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¬Ø§Ù…Ø¹")
        if analysis_data.get('customer_video'):
            strengths.append("Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ÙˆÛŒØ¯ÛŒÙˆÛŒ Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±")
        if analysis_data.get('sales_file'):
            strengths.append("Ø¯Ø§Ø´ØªÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯Ù‡Ø§")
        
        # ØªØ­Ù„ÛŒÙ„ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù
        weaknesses = []
        if conversion_rate < 40:
            weaknesses.append("Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„ Ù…Ø´ØªØ±ÛŒØ§Ù†")
        if entrance_count < 3:
            weaknesses.append("Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¯Ø± ØªØ¹Ø¯Ø§Ø¯ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡")
        if checkout_count < 4:
            weaknesses.append("Ø¸Ø±ÙÛŒØª Ù†Ø§Ú©Ø§ÙÛŒ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª Ø¯Ø± Ø³Ø§Ø¹Ø§Øª Ø´Ù„ÙˆØºÛŒ")
        if customer_dwell_time < 30:
            weaknesses.append("Ú©ÙˆØªØ§Ù‡ Ø¨ÙˆØ¯Ù† Ø²Ù…Ø§Ù† Ø­Ø¶ÙˆØ± Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¯Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡")
        if unused_area_size > store_size * 0.2:
            weaknesses.append(f"Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø§Ù†Ø¯Ù† Ø­Ø¯ÙˆØ¯ {int(unused_area_size/store_size*100)}% Ø§Ø² ÙØ¶Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡")
        if not has_surveillance:
            weaknesses.append("Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø³ÛŒØ³ØªÙ… Ù†Ø¸Ø§Ø±Øª Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒØ§Ù†")
        if uploaded_files_count < 3:
            weaknesses.append("Ø¹Ø¯Ù… Ø§Ø±Ø§Ø¦Ù‡ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ùˆ ØªØµØ§ÙˆÛŒØ± Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚")
        if not analysis_data.get('customer_video') and not analysis_data.get('surveillance_footage'):
            weaknesses.append("Ø¹Ø¯Ù… Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ÙˆÛŒØ¯ÛŒÙˆ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒØ§Ù†")
        if not analysis_data.get('sales_file'):
            weaknesses.append("Ø¹Ø¯Ù… Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯Ù‡Ø§")
        
        # ÙØ±ØµØªâ€ŒÙ‡Ø§
        opportunities = [
            "Ø§Ù…Ú©Ø§Ù† Ø¨Ù‡Ø¨ÙˆØ¯ Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† ÙØ±ÙˆØ´Ú¯Ø§Ù‡",
            "Ø§ÙØ²Ø§ÛŒØ´ Ø²Ù…Ø§Ù† Ø­Ø¶ÙˆØ± Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø§ Ø·Ø±Ø§Ø­ÛŒ Ø¨Ù‡ØªØ± ÙØ¶Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡",
            "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø±ÛŒØ§Ù† Ø­Ø±Ú©Øª Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¯Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡",
            "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª ØµÙ Ù‡ÙˆØ´Ù…Ù†Ø¯"
        ]
        
        if unused_area_size > 0:
            opportunities.append(f"Ø§Ù…Ú©Ø§Ù† Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø² {unused_area_size} Ù…ØªØ± Ù…Ø±Ø¨Ø¹ ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡")
        
        if not has_surveillance:
            opportunities.append("Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù†Ø¸Ø§Ø±Øª Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒØ§Ù†")
        
        if daily_sales_volume > 0:
            opportunities.append("Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù‚ÛŒÙ…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´")
        
        if analysis_data.get('customer_video'):
            opportunities.append("Ø§Ù…Ú©Ø§Ù† ØªØ­Ù„ÛŒÙ„ ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ")
        if analysis_data.get('store_photos'):
            opportunities.append("Ø§Ù…Ú©Ø§Ù† ØªØ­Ù„ÛŒÙ„ ØªØµÙˆÛŒØ±ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡")
        if analysis_data.get('sales_file'):
            opportunities.append("Ø§Ù…Ú©Ø§Ù† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÙØ±ÙˆØ´ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†")
        
        # ØªÙ‡Ø¯ÛŒØ¯Ù‡Ø§
        threats = [
            "Ø±Ù‚Ø§Ø¨Øª ÙØ²Ø§ÛŒÙ†Ø¯Ù‡ Ø¨Ø§ ÙØ±ÙˆØ´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§ÙˆØ±",
            "ØªØºÛŒÛŒØ±Ø§Øª Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø¯Ø± Ø±ÙØªØ§Ø± Ø®Ø±ÛŒØ¯ Ù…Ø´ØªØ±ÛŒØ§Ù†",
            "Ø§ÙØ²Ø§ÛŒØ´ Ù…Ø³ØªÙ…Ø± Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ"
        ]
        
        if unused_area_size > store_size * 0.3:
            threats.append("Ù‡Ø¯Ø±Ø±ÙØª Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø¯Ø± ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡")
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª
        recommendations = {
            "immediate": [
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§ Ùˆ Ù…Ø­ØµÙˆÙ„Ø§Øª",
                "Ù†ØµØ¨ ØªØ§Ø¨Ù„ÙˆÙ‡Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª",
                "Ø¨Ù‡Ø¨ÙˆØ¯ Ø³ÛŒØ³ØªÙ… Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡"
            ],
            "short_term": [
                "Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª",
                "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª ØµÙ",
                "Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù‚ÛŒÙ…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª"
            ],
            "long_term": [
                "Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ ÙØ¶Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡",
                "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯ÛŒØ±ÛŒØª",
                "Ú¯Ø³ØªØ±Ø´ ÙØ¶Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ùˆ ØªÙ†ÙˆØ¹ Ù…Ø­ØµÙˆÙ„Ø§Øª"
            ]
        }
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø®Ø§Øµ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        if unused_area_size > 0:
            recommendations["immediate"].append(f"Ø¨Ø§Ø²Ø·Ø±Ø§Ø­ÛŒ Ùˆ Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø² {unused_area_size} Ù…ØªØ± Ù…Ø±Ø¨Ø¹ ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡")
        
        if not has_surveillance:
            recommendations["short_term"].append("Ù†ØµØ¨ Ø³ÛŒØ³ØªÙ… Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù†Ø¸Ø§Ø±ØªÛŒ Ùˆ Ø§Ù…Ù†ÛŒØªÛŒ")
        
        if customer_dwell_time < 30:
            recommendations["immediate"].append("Ø¨Ù‡Ø¨ÙˆØ¯ Ø·Ø±Ø§Ø­ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø­Ø±Ú©Øª Ù…Ø´ØªØ±ÛŒØ§Ù†")
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø¨Ù‡Ø¨ÙˆØ¯ (Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡)
        conversion_improvement = min(25, (50 - conversion_rate) * 1.5)  # Ø¨Ù‡Ø¨ÙˆØ¯ Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„
        traffic_improvement = min(20, (500 - customer_traffic) / 500 * 30)  # Ø¨Ù‡Ø¨ÙˆØ¯ ØªØ±Ø§ÙÛŒÚ©
        space_improvement = min(15, (unused_area_size / store_size) * 30) if unused_area_size > 0 else 0
        
        # Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒâ€ŒØªØ±
        optimization_plan = {
            "layout_optimization": f"Ø¨Ø§Ø²Ø·Ø±Ø§Ø­ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ {conversion_improvement:.1f}% Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„ (Ø§Ø² {conversion_rate}% Ø¨Ù‡ {conversion_rate + conversion_improvement:.1f}%)",
            "traffic_optimization": f"Ø¨Ù‡Ø¨ÙˆØ¯ Ø¬Ø±ÛŒØ§Ù† Ø­Ø±Ú©Øª Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ {traffic_improvement:.1f}% ØªØ±Ø§ÙÛŒÚ©",
            "space_utilization": f"Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø² {unused_area_size} Ù…ØªØ± Ù…Ø±Ø¨Ø¹ ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {space_improvement:.1f}% Ø¨Ù‡Ø¨ÙˆØ¯ ÙØ±ÙˆØ´",
            "pricing_strategy": "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù‚ÛŒÙ…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù¾ÙˆÛŒØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒØ§Ù†",
            "inventory_management": "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÛŒ ÙØ±ÙˆØ´ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªÙ‚Ø§Ø¶Ø§",
            "customer_experience": "Ø¨Ù‡Ø¨ÙˆØ¯ ØªØ¬Ø±Ø¨Ù‡ Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø§ Ø·Ø±Ø§Ø­ÛŒ Ø¨Ù‡ØªØ± Ù…Ø³ÛŒØ±Ù‡Ø§ Ùˆ Ú©Ø§Ù‡Ø´ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø±",
            "technology_integration": "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ÛŒØ§Øª ÙØ±ÙˆØ´Ú¯Ø§Ù‡"
        }
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø§Ù„ÛŒ ÙˆØ§Ù‚Ø¹ÛŒâ€ŒØªØ±
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ±ÙˆØ´ ÙØ¹Ù„ÛŒ
        current_daily_sales = customer_traffic * (conversion_rate / 100) * 15000  # Ù…ØªÙˆØ³Ø· Ø®Ø±ÛŒØ¯ 15,000 ØªÙˆÙ…Ø§Ù†
        current_monthly_sales = current_daily_sales * 30
        current_yearly_sales = current_monthly_sales * 12
        
        total_sales_improvement = conversion_improvement + traffic_improvement + space_improvement
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ±ÙˆØ´ Ø¬Ø¯ÛŒØ¯
        new_daily_sales = current_daily_sales * (1 + total_sales_improvement / 100)
        additional_monthly_sales = (new_daily_sales - current_daily_sales) * 30
        additional_yearly_sales = additional_monthly_sales * 12
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ Ùˆ ROI
        implementation_cost = current_yearly_sales * 0.15  # 15% ÙØ±ÙˆØ´ Ø³Ø§Ù„Ø§Ù†Ù‡
        operational_cost_reduction = current_yearly_sales * 0.08  # 8% Ú©Ø§Ù‡Ø´ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ
        theft_reduction = current_yearly_sales * 0.02 if not has_surveillance else 0  # 2% Ú©Ø§Ù‡Ø´ Ø³Ø±Ù‚Øª
        
        total_cost_reduction = operational_cost_reduction + theft_reduction
        net_benefit = additional_yearly_sales + total_cost_reduction - implementation_cost
        roi_percentage = (net_benefit / implementation_cost) * 100 if implementation_cost > 0 else 0
        payback_period = implementation_cost / (additional_monthly_sales + total_cost_reduction / 12) if (additional_monthly_sales + total_cost_reduction / 12) > 0 else 0
        
        financial_projections = {
            "current_daily_sales": f"{current_daily_sales:,.0f} ØªÙˆÙ…Ø§Ù†",
            "current_monthly_sales": f"{current_monthly_sales:,.0f} ØªÙˆÙ…Ø§Ù†",
            "current_yearly_sales": f"{current_yearly_sales:,.0f} ØªÙˆÙ…Ø§Ù†",
            "new_daily_sales": f"{new_daily_sales:,.0f} ØªÙˆÙ…Ø§Ù†",
            "additional_monthly_sales": f"{additional_monthly_sales:,.0f} ØªÙˆÙ…Ø§Ù†",
            "additional_yearly_sales": f"{additional_yearly_sales:,.0f} ØªÙˆÙ…Ø§Ù†",
            "sales_increase_percentage": f"{total_sales_improvement:.1f}%",
            "implementation_cost": f"{implementation_cost:,.0f} ØªÙˆÙ…Ø§Ù†",
            "cost_reduction_percentage": f"{((total_cost_reduction / current_yearly_sales) * 100):.1f}%",
            "roi_percentage": f"{roi_percentage:.1f}%",
            "payback_period_months": f"{payback_period:.1f} Ù…Ø§Ù‡",
            "net_benefit_yearly": f"{net_benefit:,.0f} ØªÙˆÙ…Ø§Ù†"
        }
        
        # Ø¬Ø¯ÙˆÙ„ Ø²Ù…Ø§Ù†ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
        implementation_timeline = {
            "phase_1": "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ùˆ Ø³ÛŒØ³ØªÙ… Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ",
            "phase_2": "Ø§ÙØ²Ø§ÛŒØ´ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª ØµÙ",
            "phase_3": "Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ ÙØ¶Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯"
        }
        
        if unused_area_size > 0:
            implementation_timeline["phase_1"] += f" Ùˆ Ø¨Ø§Ø²Ø·Ø±Ø§Ø­ÛŒ {unused_area_size} Ù…ØªØ± Ù…Ø±Ø¨Ø¹ ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡"
        
        # ØªÙˆÙ„ÛŒØ¯ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ
        features = self._extract_ml_features(analysis_data)
        practical_guide = self._generate_practical_recommendations(features)
        
        return {
            "executive_summary": f"ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {analysis_data.get('store_name', '')} Ø¨Ø§ Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„ {conversion_rate}% Ùˆ {customer_traffic} Ù…Ø´ØªØ±ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ØŒ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± ÙØ±ÙˆØ´ Ø±ÙˆØ²Ø§Ù†Ù‡â€ŒØ§ÛŒ Ù…Ø¹Ø§Ø¯Ù„ {current_daily_sales:,.0f} ØªÙˆÙ…Ø§Ù† Ø¯Ø§Ø±Ø¯. Ø¨Ø§ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† Ùˆ Ø§ÙØ²Ø§ÛŒØ´ Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ {conversion_rate + conversion_improvement:.1f}%ØŒ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø² {unused_area_size} Ù…ØªØ± Ù…Ø±Ø¨Ø¹ ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡ØŒ ÙØ±ÙˆØ´ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ù‡ {new_daily_sales:,.0f} ØªÙˆÙ…Ø§Ù† Ø§ÙØ²Ø§ÛŒØ´ Ø®ÙˆØ§Ù‡Ø¯ ÛŒØ§ÙØª. Ø§ÛŒÙ† Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ Ù…Ù†Ø¬Ø± Ø¨Ù‡ {total_sales_improvement:.1f}% Ø±Ø´Ø¯ ÙØ±ÙˆØ´ØŒ Ø¨Ø§Ø²Ø¯Ù‡ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ {roi_percentage:.1f}% Ùˆ Ø¨Ø§Ø²Ú¯Ø´Øª Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø¯Ø± Ù…Ø¯Øª {payback_period:.1f} Ù…Ø§Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.",
            "detailed_analysis": {
                "strengths": strengths,
                "weaknesses": weaknesses,
                "opportunities": opportunities,
                "threats": threats
            },
            "recommendations": recommendations,
            "optimization_plan": optimization_plan,
            "financial_projections": financial_projections,
            "implementation_timeline": implementation_timeline,
            "practical_guide": practical_guide,
            "generated_at": datetime.now().isoformat(),
            "ai_model": "local_analysis",
            "confidence_score": 0.85
        }
    
    def _prepare_store_info(self, analysis_data: Dict[str, Any]) -> str:
        """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø¨Ø±Ø§ÛŒ AI"""
        info_parts = []
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡
        info_parts.append(f"Ù†Ø§Ù… ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {analysis_data.get('store_name', 'Ù†Ø§Ù…Ø´Ø®Øµ')}")
        info_parts.append(f"Ù†ÙˆØ¹ ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {analysis_data.get('store_type', 'Ù†Ø§Ù…Ø´Ø®Øµ')}")
        info_parts.append(f"Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {analysis_data.get('store_size', 'Ù†Ø§Ù…Ø´Ø®Øµ')} Ù…ØªØ± Ù…Ø±Ø¨Ø¹")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ
        if analysis_data.get('store_location'):
            info_parts.append(f"Ø¢Ø¯Ø±Ø³: {analysis_data.get('store_location')}")
        if analysis_data.get('city'):
            info_parts.append(f"Ø´Ù‡Ø±: {analysis_data.get('city')}")
        if analysis_data.get('area'):
            info_parts.append(f"Ù…Ù†Ø·Ù‚Ù‡: {analysis_data.get('area')}")
        if analysis_data.get('establishment_year'):
            info_parts.append(f"Ø³Ø§Ù„ ØªØ§Ø³ÛŒØ³: {analysis_data.get('establishment_year')}")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙÛŒØ²ÛŒÚ©ÛŒ
        info_parts.append(f"ØªØ¹Ø¯Ø§Ø¯ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§: {analysis_data.get('entrance_count', 0)}")
        info_parts.append(f"ØªØ¹Ø¯Ø§Ø¯ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§: {analysis_data.get('checkout_count', 0)}")
        info_parts.append(f"ØªØ¹Ø¯Ø§Ø¯ Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§: {analysis_data.get('shelf_count', 0)}")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ú†ÛŒØ¯Ù…Ø§Ù†
        if analysis_data.get('shelf_dimensions'):
            info_parts.append(f"Ø§Ø¨Ø¹Ø§Ø¯ Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§: {analysis_data.get('shelf_dimensions')}")
        if analysis_data.get('shelf_contents'):
            info_parts.append(f"Ù…Ø­ØªÙˆØ§ÛŒ Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§: {analysis_data.get('shelf_contents')}")
        if analysis_data.get('unused_area_size'):
            info_parts.append(f"Ù…Ù†Ø§Ø·Ù‚ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡: {analysis_data.get('unused_area_size')} Ù…ØªØ± Ù…Ø±Ø¨Ø¹")
        if analysis_data.get('unused_area_type'):
            info_parts.append(f"Ù†ÙˆØ¹ Ù…Ù†Ø§Ø·Ù‚ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡: {analysis_data.get('unused_area_type')}")
        
        # Ø·Ø±Ø§Ø­ÛŒ Ùˆ Ø¯Ú©ÙˆØ±Ø§Ø³ÛŒÙˆÙ†
        if analysis_data.get('design_style'):
            info_parts.append(f"Ø³Ø¨Ú© Ø·Ø±Ø§Ø­ÛŒ: {analysis_data.get('design_style')}")
        if analysis_data.get('brand_colors'):
            info_parts.append(f"Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ù†Ø¯: {analysis_data.get('brand_colors')}")
        info_parts.append(f"Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ Ø§ØµÙ„ÛŒ: {analysis_data.get('main_lighting', 'Ù†Ø§Ù…Ø´Ø®Øµ')}")
        if analysis_data.get('lighting_intensity'):
            info_parts.append(f"Ø´Ø¯Øª Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ: {analysis_data.get('lighting_intensity')}")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¹Ù…Ù„Ú©Ø±Ø¯
        info_parts.append(f"Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„: {analysis_data.get('conversion_rate', 0)}%")
        info_parts.append(f"Ù…ØªÙˆØ³Ø· Ù…Ø´ØªØ±ÛŒØ§Ù† Ø±ÙˆØ²Ø§Ù†Ù‡: {analysis_data.get('customer_traffic', 0)}")
        info_parts.append(f"Ù…ØªÙˆØ³Ø· Ø²Ù…Ø§Ù† Ø­Ø¶ÙˆØ± Ù…Ø´ØªØ±ÛŒ: {analysis_data.get('customer_dwell_time', 0)} Ø¯Ù‚ÛŒÙ‚Ù‡")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ±Ø§ÙÛŒÚ© Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
        if analysis_data.get('peak_hours'):
            info_parts.append(f"Ø³Ø§Ø¹Ø§Øª Ù¾ÛŒÚ©: {analysis_data.get('peak_hours')}")
        if analysis_data.get('high_traffic_areas'):
            info_parts.append(f"Ù…Ù†Ø§Ø·Ù‚ Ù¾Ø±ØªØ±Ø¯Ø¯: {analysis_data.get('high_traffic_areas')}")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ±ÙˆØ´
        info_parts.append(f"Ø¯Ø±ØµØ¯ ÙØ±ÙˆØ´ ØµØ¨Ø­: {analysis_data.get('morning_sales_percent', 0)}%")
        info_parts.append(f"Ø¯Ø±ØµØ¯ ÙØ±ÙˆØ´ Ø¸Ù‡Ø±: {analysis_data.get('noon_sales_percent', 0)}%")
        info_parts.append(f"Ø¯Ø±ØµØ¯ ÙØ±ÙˆØ´ Ø´Ø¨: {analysis_data.get('evening_sales_percent', 0)}%")
        
        # Ù…Ø­ØµÙˆÙ„Ø§Øª Ùˆ ÙØ±ÙˆØ´
        if analysis_data.get('product_categories'):
            info_parts.append(f"Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª: {', '.join(analysis_data.get('product_categories', []))}")
        if analysis_data.get('top_products'):
            info_parts.append(f"Ù…Ø­ØµÙˆÙ„Ø§Øª Ù¾Ø±ÙØ±ÙˆØ´: {analysis_data.get('top_products')}")
        if analysis_data.get('daily_sales_volume'):
            info_parts.append(f"ÙØ±ÙˆØ´ Ø±ÙˆØ²Ø§Ù†Ù‡: {analysis_data.get('daily_sales_volume')} ØªÙˆÙ…Ø§Ù†")
        if analysis_data.get('supplier_count'):
            info_parts.append(f"ØªØ¹Ø¯Ø§Ø¯ ØªØ§Ù…ÛŒÙ†â€ŒÚ©Ù†Ù†Ø¯Ú¯Ø§Ù†: {analysis_data.get('supplier_count')}")
        
        # Ù†Ø¸Ø§Ø±Øª Ùˆ Ø§Ù…Ù†ÛŒØª
        if analysis_data.get('has_surveillance'):
            info_parts.append(f"Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù†Ø¸Ø§Ø±ØªÛŒ: Ø¨Ù„Ù‡")
            if analysis_data.get('camera_count'):
                info_parts.append(f"ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ø¨ÛŒÙ†â€ŒÙ‡Ø§: {analysis_data.get('camera_count')}")
            if analysis_data.get('camera_locations'):
                info_parts.append(f"Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¯ÙˆØ±Ø¨ÛŒÙ†â€ŒÙ‡Ø§: {analysis_data.get('camera_locations')}")
        else:
            info_parts.append(f"Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù†Ø¸Ø§Ø±ØªÛŒ: Ø®ÛŒØ±")
        
        # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ
        if analysis_data.get('pos_system'):
            info_parts.append(f"Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± ØµÙ†Ø¯ÙˆÙ‚: {analysis_data.get('pos_system')}")
        if analysis_data.get('inventory_system'):
            info_parts.append(f"Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ù…ÙˆØ¬ÙˆØ¯ÛŒ: {analysis_data.get('inventory_system')}")
        if analysis_data.get('video_date'):
            info_parts.append(f"ØªØ§Ø±ÛŒØ® Ø¶Ø¨Ø· ÙˆÛŒØ¯ÛŒÙˆ: {analysis_data.get('video_date')}")
        if analysis_data.get('video_duration'):
            info_parts.append(f"Ù…Ø¯Øª ÙˆÛŒØ¯ÛŒÙˆ: {analysis_data.get('video_duration')} Ø«Ø§Ù†ÛŒÙ‡")
        
        # Ù†ÙˆØ¹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡
        uploaded_files = []
        if analysis_data.get('store_photos'):
            uploaded_files.append("ØªØµØ§ÙˆÛŒØ± ÙØ±ÙˆØ´Ú¯Ø§Ù‡")
        if analysis_data.get('store_plan'):
            uploaded_files.append("Ù†Ù‚Ø´Ù‡ ÙØ±ÙˆØ´Ú¯Ø§Ù‡")
        if analysis_data.get('shelf_photos'):
            uploaded_files.append("ØªØµØ§ÙˆÛŒØ± Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§")
        if analysis_data.get('entrance_photos'):
            uploaded_files.append("ØªØµØ§ÙˆÛŒØ± ÙˆØ±ÙˆØ¯ÛŒ")
        if analysis_data.get('checkout_photos'):
            uploaded_files.append("ØªØµØ§ÙˆÛŒØ± ØµÙ†Ø¯ÙˆÙ‚")
        if analysis_data.get('customer_video'):
            uploaded_files.append("ÙˆÛŒØ¯ÛŒÙˆÛŒ Ù…Ø´ØªØ±ÛŒØ§Ù†")
        if analysis_data.get('surveillance_footage'):
            uploaded_files.append("ÙÛŒÙ„Ù… Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù†Ø¸Ø§Ø±ØªÛŒ")
        if analysis_data.get('sales_file'):
            uploaded_files.append("ÙØ§ÛŒÙ„ ÙØ±ÙˆØ´")
        if analysis_data.get('product_catalog'):
            uploaded_files.append("Ú©Ø§ØªØ§Ù„ÙˆÚ¯ Ù…Ø­ØµÙˆÙ„Ø§Øª")
        
        if uploaded_files:
            info_parts.append(f"ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡: {', '.join(uploaded_files)}")
        
        return "\n".join(info_parts)
    
    def generate_implementation_guide(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„ÛŒ"""
        
        guide = {
            "title": "Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡",
            "overview": "Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§ Ø´Ø§Ù…Ù„ Ù…Ø±Ø§Ø­Ù„ Ø¹Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª ØªØ­Ù„ÛŒÙ„ Ø§Ø³Øª.",
            "phases": {},
            "checklist": {},
            "resources": {},
            "timeline": {}
        }
        
        # ÙØ§Ø² Ø§ÙˆÙ„ (1-2 Ù…Ø§Ù‡)
        guide["phases"]["phase_1"] = {
            "title": "ÙØ§Ø² Ø§ÙˆÙ„: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø±ÛŒØ¹",
            "duration": "1-2 Ù…Ø§Ù‡",
            "budget": "Ú©Ù…",
            "priority": "Ø¨Ø§Ù„Ø§",
            "tasks": [
                "Ø¨Ø§Ø²Ø·Ø±Ø§Ø­ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§",
                "Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ",
                "Ù†ØµØ¨ ØªØ§Ø¨Ù„ÙˆÙ‡Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§",
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…Ø´ØªØ±ÛŒ"
            ],
            "expected_results": [
                "Ø§ÙØ²Ø§ÛŒØ´ 5-10% Ù†Ø±Ø® ØªØ¨Ø¯ÛŒÙ„",
                "Ú©Ø§Ù‡Ø´ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø±",
                "Ø¨Ù‡Ø¨ÙˆØ¯ ØªØ¬Ø±Ø¨Ù‡ Ù…Ø´ØªØ±ÛŒ"
            ]
        }
        
        # ÙØ§Ø² Ø¯ÙˆÙ… (3-6 Ù…Ø§Ù‡)
        guide["phases"]["phase_2"] = {
            "title": "ÙØ§Ø² Ø¯ÙˆÙ…: Ø¨Ù‡Ø¨ÙˆØ¯ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§",
            "duration": "3-6 Ù…Ø§Ù‡",
            "budget": "Ù…ØªÙˆØ³Ø·",
            "priority": "Ù…ØªÙˆØ³Ø·",
            "tasks": [
                "Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§",
                "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª ØµÙ",
                "Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù‚ÛŒÙ…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ",
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒ"
            ],
            "expected_results": [
                "Ø§ÙØ²Ø§ÛŒØ´ 15-20% ÙØ±ÙˆØ´",
                "Ú©Ø§Ù‡Ø´ 20% Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ",
                "Ø¨Ù‡Ø¨ÙˆØ¯ Ø±Ø¶Ø§ÛŒØª Ú©Ø§Ø±Ú©Ù†Ø§Ù†"
            ]
        }
        
        # ÙØ§Ø² Ø³ÙˆÙ… (6-12 Ù…Ø§Ù‡)
        guide["phases"]["phase_3"] = {
            "title": "ÙØ§Ø² Ø³ÙˆÙ…: ØªØ­ÙˆÙ„ Ø¯ÛŒØ¬ÛŒØªØ§Ù„",
            "duration": "6-12 Ù…Ø§Ù‡",
            "budget": "Ø¨Ø§Ù„Ø§",
            "priority": "Ú©Ù…",
            "tasks": [
                "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯",
                "Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡",
                "Ú¯Ø³ØªØ±Ø´ ÙØ¶Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡",
                "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØ¬Ø§Ø±Øª Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©"
            ],
            "expected_results": [
                "Ø§ÙØ²Ø§ÛŒØ´ 25-30% ÙØ±ÙˆØ´",
                "Ú©Ø§Ù‡Ø´ 30% Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§",
                "Ø±Ù‚Ø§Ø¨Øªâ€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨Ø§Ù„Ø§"
            ]
        }
        
        # Ú†Ú©â€ŒÙ„ÛŒØ³Øª Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
        guide["checklist"] = {
            "pre_implementation": [
                "ØªØ£ÛŒÛŒØ¯ Ø¨ÙˆØ¯Ø¬Ù‡",
                "ØªØ´Ú©ÛŒÙ„ ØªÛŒÙ… Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ",
                "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø²Ù…Ø§Ù†ÛŒ",
                "Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ø±Ú©Ù†Ø§Ù†"
            ],
            "during_implementation": [
                "Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ù¾ÛŒØ´Ø±ÙØª",
                "Ù…Ø¯ÛŒØ±ÛŒØª ØªØºÛŒÛŒØ±Ø§Øª",
                "Ø­Ù„ Ù…Ø´Ú©Ù„Ø§Øª",
                "Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø°ÛŒÙ†ÙØ¹Ø§Ù†"
            ],
            "post_implementation": [
                "Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†ØªØ§ÛŒØ¬",
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§",
                "Ø¢Ù…ÙˆØ²Ø´ Ù…Ø³ØªÙ…Ø±",
                "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡"
            ]
        }
        
        # Ù…Ù†Ø§Ø¨Ø¹ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
        guide["resources"] = {
            "human_resources": [
                "Ù…Ø¯ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡",
                "Ù…ØªØ®ØµØµ Ø·Ø±Ø§Ø­ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡",
                "Ú©Ø§Ø±Ø´Ù†Ø§Ø³ IT",
                "Ú©Ø§Ø±Ú©Ù†Ø§Ù† ÙØ±ÙˆØ´Ú¯Ø§Ù‡"
            ],
            "technical_resources": [
                "Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø·Ø±Ø§Ø­ÛŒ",
                "Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª ØµÙ",
                "ØªØ¬Ù‡ÛŒØ²Ø§Øª Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ",
                "ØªØ§Ø¨Ù„ÙˆÙ‡Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§"
            ],
            "financial_resources": [
                "Ø¨ÙˆØ¯Ø¬Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ",
                "Ø¨ÙˆØ¯Ø¬Ù‡ Ø¢Ù…ÙˆØ²Ø´",
                "Ø¨ÙˆØ¯Ø¬Ù‡ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ",
                "Ø¨ÙˆØ¯Ø¬Ù‡ Ø§Ø¶Ø·Ø±Ø§Ø±ÛŒ"
            ]
        }
        
        # Ø¬Ø¯ÙˆÙ„ Ø²Ù…Ø§Ù†ÛŒ
        guide["timeline"] = {
            "week_1_2": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ",
            "week_3_4": "Ø´Ø±ÙˆØ¹ ÙØ§Ø² Ø§ÙˆÙ„",
            "month_2": "ØªÚ©Ù…ÛŒÙ„ ÙØ§Ø² Ø§ÙˆÙ„",
            "month_3_4": "Ø´Ø±ÙˆØ¹ ÙØ§Ø² Ø¯ÙˆÙ…",
            "month_5_6": "ØªÚ©Ù…ÛŒÙ„ ÙØ§Ø² Ø¯ÙˆÙ…",
            "month_7_12": "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§Ø² Ø³ÙˆÙ…"
        }
        
        return guide

# Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡
if __name__ == "__main__":
    # ØªØ³Øª Ø³ÛŒØ³ØªÙ…
    ai_analyzer = StoreAnalysisAI()
    
    test_data = {
        'store_name': 'ÙØ±ÙˆØ´Ú¯Ø§Ù‡ ØªØ³Øª',
        'store_type': 'retail',
        'store_size': 'medium',
        'entrance_count': 2,
        'checkout_count': 3,
        'shelf_count': 25,
        'conversion_rate': 35.5,
        'avg_daily_customers': 150,
        'avg_customer_time': 45
    }
    
    result = ai_analyzer.generate_detailed_analysis(test_data)
    print(json.dumps(result, indent=2, ensure_ascii=False))
