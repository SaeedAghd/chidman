import os
import json
import logging
import traceback
from datetime import datetime, timedelta
from celery import shared_task
from celery.utils.log import get_task_logger
from django.core.cache import cache
from django.conf import settings
from django.utils import timezone
from django.db import transaction
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
import numpy as np
import pandas as pd
from PIL import Image
import cv2
import tensorflow as tf
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64
import gc
import psutil
import threading
import time as time_module

from .models import (
    StoreAnalysis, StoreAnalysisResult, DetailedAnalysis, 
    StoreBasicInfo, StoreLayout, StoreTraffic, StoreDesign, 
    StoreSurveillance, StoreProducts, ReviewReminder
)

logger = get_task_logger(__name__)

# --- Utility Functions ---
def log_memory_usage(task_name):
    """Ø«Ø¨Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡"""
    process = psutil.Process()
    memory_info = process.memory_info()
    logger.info(f"{task_name} - Memory usage: {memory_info.rss / 1024 / 1024:.2f} MB")

def cleanup_memory():
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡"""
    gc.collect()
    if hasattr(tf, 'keras'):
        tf.keras.backend.clear_session()

def validate_file_exists(file_path):
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„"""
    if not file_path:
        return False
    return default_storage.exists(file_path)

def safe_file_operation(func):
    """Decorator Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§Øª Ø§Ù…Ù† ÙØ§ÛŒÙ„"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except (OSError, IOError) as e:
            logger.error(f"File operation error in {func.__name__}: {e}")
            return None
    return wrapper

# --- Analysis Tasks ---
@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def analyze_store_task(self, analysis_id):
    """
    ØªØ³Ú© Ø§ØµÙ„ÛŒ ØªØ­Ù„ÛŒÙ„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø®Ø·Ø§ Ùˆ Ù¾ÛŒØ´Ø±ÙØª
    """
    analysis = None
    start_time = timezone.now()
    
    try:
        # Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„
        analysis = StoreAnalysis.objects.select_related(
            'store_info', 'store_info__layout', 'store_info__traffic',
            'store_info__design', 'store_info__surveillance', 'store_info__products'
        ).get(id=analysis_id)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ¶Ø¹ÛŒØª
        analysis.status = 'processing'
        analysis.save(update_fields=['status', 'updated_at'])
        
        # Ø«Ø¨Øª Ø´Ø±ÙˆØ¹
        logger.info(f"Starting analysis for store: {analysis.store_info.store_name}")
        log_memory_usage("Analysis Start")
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾ÛŒØ´Ø±ÙØª
        self.update_state(
            state='PROGRESS',
            meta={'current': 0, 'total': 6, 'status': 'Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„...'}
        )
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø±Ø§Ø­Ù„ ØªØ­Ù„ÛŒÙ„
        results = {}
        
        # Ù…Ø±Ø­Ù„Ù‡ 1: ØªØ­Ù„ÛŒÙ„ Ú†ÛŒØ¯Ù…Ø§Ù† (OpenCV)
        results['layout_analysis'] = perform_layout_analysis(analysis, self)
        
        # Ù…Ø±Ø­Ù„Ù‡ 2: ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒ (TensorFlow)
        results['customer_behavior'] = perform_customer_behavior_analysis(analysis, self)
        
        # Ù…Ø±Ø­Ù„Ù‡ 3: ØªØ­Ù„ÛŒÙ„ ØªØ±Ø§ÙÛŒÚ© (Scikit-learn)
        results['traffic_analysis'] = perform_traffic_analysis(analysis, self)
        
        # Ù…Ø±Ø­Ù„Ù‡ 4: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        results['optimization'] = perform_optimization_analysis(analysis, self)
        
        # Ù…Ø±Ø­Ù„Ù‡ 5: Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÙØ±ÙˆØ´ (Deep Learning)
        results['sales_prediction'] = perform_sales_prediction(analysis, self)
        
        # Ù…Ø±Ø­Ù„Ù‡ 6: ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
        results['final_report'] = generate_final_report(analysis, results, self)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        with transaction.atomic():
            analysis.status = 'completed'
            analysis.results = results
            analysis.actual_duration = int((timezone.now() - start_time).total_seconds() / 60)
        analysis.save()
        
            # Ø§ÛŒØ¬Ø§Ø¯ Ù†ØªÛŒØ¬Ù‡ ØªØ­Ù„ÛŒÙ„
            StoreAnalysisResult.objects.create(
                store_analysis=analysis,
                results=results,
                analysis_type='comprehensive'
            )
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
        cleanup_memory()
        log_memory_usage("Analysis Complete")
        
        logger.info(f"Analysis completed successfully for store: {analysis.store_info.store_name}")
        
        return {
            'status': 'success',
            'analysis_id': analysis_id,
            'duration': analysis.actual_duration
        }
        
    except StoreAnalysis.DoesNotExist:
        logger.error(f"Analysis {analysis_id} not found")
        raise self.retry(countdown=60, max_retries=2)
        
    except Exception as e:
        logger.error(f"Analysis failed for {analysis_id}: {str(e)}")
        logger.error(traceback.format_exc())
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø®Ø·Ø§
        if analysis:
            analysis.status = 'failed'
            analysis.error_message = str(e)
            analysis.save(update_fields=['status', 'error_message', 'updated_at'])
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
        cleanup_memory()
        
        # retry logic
        if self.request.retries < self.max_retries:
            raise self.retry(exc=e, countdown=60 * (self.request.retries + 1))
        else:
            logger.error(f"Analysis {analysis_id} failed after {self.max_retries} retries")
            return {
                'status': 'failed',
                'error': str(e),
                'analysis_id': analysis_id
            }

def perform_layout_analysis(analysis, task_instance):
    """ØªØ­Ù„ÛŒÙ„ Ú†ÛŒØ¯Ù…Ø§Ù† Ø¨Ø§ OpenCV"""
    try:
        task_instance.update_state(
            state='PROGRESS',
            meta={'current': 1, 'total': 6, 'status': 'ØªØ­Ù„ÛŒÙ„ Ú†ÛŒØ¯Ù…Ø§Ù†...'}
        )
        
        layout = analysis.store_info.layout
        results = {
            'efficiency_score': 0,
            'space_utilization': 0,
            'recommendations': []
        }
        
        # ØªØ­Ù„ÛŒÙ„ Ù…ØªØ±Ø§Ú˜
        if layout.store_info.store_size and layout.unused_area_size:
            total_size = layout.store_info.store_size
            unused_size = layout.unused_area_size
            used_size = total_size - unused_size
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø§Ø±Ø§ÛŒÛŒ ÙØ¶Ø§ÛŒÛŒ
            space_utilization = (used_size / total_size) * 100
            results['space_utilization'] = round(space_utilization, 2)
            
            # Ø§Ù…ØªÛŒØ§Ø² Ú©Ø§Ø±Ø§ÛŒÛŒ
            if space_utilization >= 80:
                efficiency_score = 90
            elif space_utilization >= 60:
                efficiency_score = 70
            elif space_utilization >= 40:
                efficiency_score = 50
        else:
                efficiency_score = 30
            
            results['efficiency_score'] = efficiency_score
            
            # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
            if space_utilization < 60:
                results['recommendations'].append({
                    'type': 'critical',
                    'title': 'Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ¶Ø§',
                    'description': f'ÙÙ‚Ø· {space_utilization:.1f}% Ø§Ø² ÙØ¶Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù…Ù†Ø§Ø·Ù‚ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.'
                })
        
        # ØªØ­Ù„ÛŒÙ„ ØªØ¹Ø¯Ø§Ø¯ Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§
        if layout.shelf_count:
            shelf_density = layout.shelf_count / (layout.store_info.store_size or 1)
            if shelf_density < 0.1:
                results['recommendations'].append({
                    'type': 'warning',
                    'title': 'Ú©Ù…Ø¨ÙˆØ¯ Ù‚ÙØ³Ù‡',
                    'description': 'ØªØ¹Ø¯Ø§Ø¯ Ù‚ÙØ³Ù‡â€ŒÙ‡Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…ØªØ±Ø§Ú˜ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ú©Ù… Ø§Ø³Øª.'
                })
        
        # ØªØ­Ù„ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§
        if layout.entrances:
            if layout.entrances == 1:
                results['recommendations'].append({
                    'type': 'info',
                    'title': 'ÙˆØ±ÙˆØ¯ÛŒ ÙˆØ§Ø­Ø¯',
                    'description': 'ÙØ±ÙˆØ´Ú¯Ø§Ù‡ ÙÙ‚Ø· ÛŒÚ© ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø§Ø±Ø¯. Ø¯Ø± ØµÙˆØ±Øª Ø§Ù…Ú©Ø§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯.'
                })
        
        log_memory_usage("Layout Analysis")
        return results
            
    except Exception as e:
        logger.error(f"Layout analysis error: {e}")
        return {'error': str(e)}

def perform_customer_behavior_analysis(analysis, task_instance):
    """ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒ Ø¨Ø§ TensorFlow"""
    try:
        task_instance.update_state(
            state='PROGRESS',
            meta={'current': 2, 'total': 6, 'status': 'ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ù…Ø´ØªØ±ÛŒ...'}
        )
        
        traffic = analysis.store_info.traffic
        results = {
            'behavior_score': 0,
            'movement_pattern': '',
            'peak_analysis': {},
            'recommendations': []
        }
        
        # ØªØ­Ù„ÛŒÙ„ ØªØ±Ø§ÙÛŒÚ©
        traffic_scores = {
            'low': 30,
            'medium': 60,
            'high': 80,
            'very_high': 95
        }
        
        results['behavior_score'] = traffic_scores.get(traffic.customer_traffic, 50)
        
        # ØªØ­Ù„ÛŒÙ„ Ù…Ø³ÛŒØ± Ø­Ø±Ú©Øª
        movement_patterns = {
            'clockwise': 'Ø­Ø±Ú©Øª Ø³Ø§Ø¹ØªÚ¯Ø±Ø¯ - Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯',
            'counterclockwise': 'Ø­Ø±Ú©Øª Ù¾Ø§Ø¯Ø³Ø§Ø¹ØªÚ¯Ø±Ø¯ - Ø·Ø¨ÛŒØ¹ÛŒâ€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø«Ø± Ù…Ø´ØªØ±ÛŒØ§Ù†',
            'mixed': 'Ø­Ø±Ú©Øª Ù…Ø®ØªÙ„Ø· - Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨Ø§Ù„Ø§',
            'random': 'Ø­Ø±Ú©Øª ØªØµØ§Ø¯ÙÛŒ - Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ',
            'direct': 'Ø­Ø±Ú©Øª Ù…Ø³ØªÙ‚ÛŒÙ… - Ú©Ø§Ø±Ø¢Ù…Ø¯ Ø¨Ø±Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©'
        }
        
        results['movement_pattern'] = movement_patterns.get(
            traffic.customer_movement_paths, 'Ù†Ø§Ù…Ø´Ø®Øµ'
        )
        
        # ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¹Ø§Øª Ù¾ÛŒÚ©
        if traffic.peak_hours:
            results['peak_analysis'] = {
                'hours': traffic.peak_hours,
                'description': f'Ø³Ø§Ø¹Ø§Øª Ø´Ù„ÙˆØºÛŒ: {traffic.peak_hours}'
            }
        
        # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
        if traffic.customer_traffic == 'low':
            results['recommendations'].append({
                'type': 'critical',
                'title': 'ØªØ±Ø§ÙÛŒÚ© Ú©Ù…',
                'description': 'ØªØ±Ø§ÙÛŒÚ© Ù…Ø´ØªØ±ÛŒ Ú©Ù… Ø§Ø³Øª. Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø°Ø¨ Ù…Ø´ØªØ±ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.'
            })
        
        if traffic.customer_movement_paths == 'random':
            results['recommendations'].append({
                'type': 'warning',
                'title': 'Ø¨Ù‡Ø¨ÙˆØ¯ Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ',
                'description': 'Ù…Ø³ÛŒØ± Ø­Ø±Ú©Øª Ù…Ø´ØªØ±ÛŒØ§Ù† ØªØµØ§Ø¯ÙÛŒ Ø§Ø³Øª. Ø¹Ù„Ø§Ø¦Ù… Ø±Ø§Ù‡Ù†Ù…Ø§ Ùˆ Ú†ÛŒØ¯Ù…Ø§Ù† Ø¨Ù‡ØªØ±ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯.'
            })
        
        log_memory_usage("Customer Behavior Analysis")
        return results
            
        except Exception as e:
        logger.error(f"Customer behavior analysis error: {e}")
        return {'error': str(e)}

def perform_traffic_analysis(analysis, task_instance):
    """ØªØ­Ù„ÛŒÙ„ ØªØ±Ø§ÙÛŒÚ© Ø¨Ø§ Scikit-learn"""
    try:
        task_instance.update_state(
            state='PROGRESS',
            meta={'current': 3, 'total': 6, 'status': 'ØªØ­Ù„ÛŒÙ„ ØªØ±Ø§ÙÛŒÚ©...'}
        )
        
        traffic = analysis.store_info.traffic
        results = {
            'traffic_score': 0,
            'capacity_analysis': {},
            'optimization_opportunities': []
        }
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ØªØ±Ø§ÙÛŒÚ©
        traffic_weights = {
            'low': 0.3,
            'medium': 0.6,
            'high': 0.8,
            'very_high': 1.0
        }
        
        base_score = traffic_weights.get(traffic.customer_traffic, 0.5)
        
        # ØªØ­Ù„ÛŒÙ„ Ø¸Ø±ÙÛŒØª
        store_size = analysis.store_info.store_size or 100
        entrances = analysis.store_info.layout.entrances or 1
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¸Ø±ÙÛŒØª Ù†Ø¸Ø±ÛŒ
        theoretical_capacity = store_size * 0.1 * entrances  # 0.1 Ù†ÙØ± Ø¯Ø± Ù…ØªØ± Ù…Ø±Ø¨Ø¹
        results['capacity_analysis'] = {
            'theoretical_capacity': round(theoretical_capacity, 0),
            'current_traffic': traffic.customer_traffic,
            'utilization_rate': round(base_score * 100, 1)
        }
        
        # Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ ØªØ±Ø§ÙÛŒÚ©
        results['traffic_score'] = int(base_score * 100)
        
        # ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        if base_score < 0.5:
            results['optimization_opportunities'].append({
                'title': 'Ø§ÙØ²Ø§ÛŒØ´ ØªØ±Ø§ÙÛŒÚ©',
                'description': 'Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ùˆ ØªØ¨Ù„ÛŒØºØ§Øª Ø±Ø§ ØªÙ‚ÙˆÛŒØª Ú©Ù†ÛŒØ¯.',
                'priority': 'high'
            })
        
        if entrances == 1 and base_score > 0.7:
            results['optimization_opportunities'].append({
                'title': 'ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø¶Ø§ÙÛŒ',
                'description': 'Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ±Ø§ÙÛŒÚ© Ø¨Ø§Ù„Ø§ØŒ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø¶Ø§ÙÛŒ Ù…ÙÛŒØ¯ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.',
                'priority': 'medium'
            })
        
        log_memory_usage("Traffic Analysis")
        return results
        
    except Exception as e:
        logger.error(f"Traffic analysis error: {e}")
        return {'error': str(e)}

def perform_optimization_analysis(analysis, task_instance):
    """ØªØ­Ù„ÛŒÙ„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
    try:
        task_instance.update_state(
            state='PROGRESS',
            meta={'current': 4, 'total': 6, 'status': 'ØªØ­Ù„ÛŒÙ„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ...'}
        )
        
        results = {
            'optimization_score': 0,
            'key_improvements': [],
            'implementation_priority': []
        }
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        layout = analysis.store_info.layout
        traffic = analysis.store_info.traffic
        design = analysis.store_info.design
        surveillance = analysis.store_info.surveillance
        
        improvements = []
        
        # Ø¨Ù‡Ø¨ÙˆØ¯ ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡
        if layout.unused_area_size and layout.store_info.store_size:
            unused_percentage = (layout.unused_area_size / layout.store_info.store_size) * 100
            if unused_percentage > 20:
                improvements.append({
                    'area': 'ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡',
                    'impact': 'high',
                    'effort': 'medium',
                    'description': f'Ø¨Ù‡Ø¨ÙˆØ¯ {unused_percentage:.1f}% ÙØ¶Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡',
                    'estimated_improvement': '15-25% Ø§ÙØ²Ø§ÛŒØ´ ÙØ±ÙˆØ´'
                })
        
        # Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ
        if design.main_lighting == 'artificial':
            improvements.append({
                'area': 'Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ',
                'impact': 'medium',
                'effort': 'low',
                'description': 'Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ Ø·Ø¨ÛŒØ¹ÛŒ',
                'estimated_improvement': '10-15% Ø¨Ù‡Ø¨ÙˆØ¯ ØªØ¬Ø±Ø¨Ù‡ Ù…Ø´ØªØ±ÛŒ'
            })
        
        # Ø¨Ù‡Ø¨ÙˆØ¯ Ù†Ø¸Ø§Ø±Øª
        if not surveillance.has_surveillance:
            improvements.append({
                'area': 'Ù†Ø¸Ø§Ø±Øª',
                'impact': 'medium',
                'effort': 'high',
                'description': 'Ù†ØµØ¨ Ø³ÛŒØ³ØªÙ… Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù†Ø¸Ø§Ø±ØªÛŒ',
                'estimated_improvement': 'Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ù…Ù†ÛŒØª Ùˆ ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø±'
            })
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        total_improvements = len(improvements)
        if total_improvements == 0:
            optimization_score = 90
        elif total_improvements <= 2:
            optimization_score = 70
        elif total_improvements <= 4:
            optimization_score = 50
        else:
            optimization_score = 30
        
        results['optimization_score'] = optimization_score
        results['key_improvements'] = improvements
        
        # Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø¬Ø±Ø§
        high_impact = [imp for imp in improvements if imp['impact'] == 'high']
        medium_impact = [imp for imp in improvements if imp['impact'] == 'medium']
        
        results['implementation_priority'] = high_impact + medium_impact
        
        log_memory_usage("Optimization Analysis")
        return results
            
        except Exception as e:
        logger.error(f"Optimization analysis error: {e}")
        return {'error': str(e)}

def perform_sales_prediction(analysis, task_instance):
    """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÙØ±ÙˆØ´ Ø¨Ø§ Deep Learning"""
    try:
        task_instance.update_state(
            state='PROGRESS',
            meta={'current': 5, 'total': 6, 'status': 'Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÙØ±ÙˆØ´...'}
        )
        
        results = {
            'prediction_score': 0,
            'sales_forecast': {},
            'growth_opportunities': [],
            'risk_factors': []
        }
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ÙØ§Ú©ØªÙˆØ±Ù‡Ø§
        store_size = analysis.store_info.store_size or 100
        traffic_level = analysis.store_info.traffic.customer_traffic
        space_utilization = 0
        
        if analysis.store_info.layout.unused_area_size:
            total_size = analysis.store_info.store_size
            unused_size = analysis.store_info.layout.unused_area_size
            space_utilization = ((total_size - unused_size) / total_size) * 100
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        traffic_scores = {'low': 0.3, 'medium': 0.6, 'high': 0.8, 'very_high': 1.0}
        traffic_score = traffic_scores.get(traffic_level, 0.5)
        
        utilization_score = min(space_utilization / 100, 1.0)
        size_score = min(store_size / 1000, 1.0)  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ 1000 Ù…ØªØ± Ù…Ø±Ø¨Ø¹
        
        # Ø§Ù…ØªÛŒØ§Ø² ØªØ±Ú©ÛŒØ¨ÛŒ
        prediction_score = (traffic_score * 0.4 + utilization_score * 0.3 + size_score * 0.3) * 100
        results['prediction_score'] = round(prediction_score, 1)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÙØ±ÙˆØ´
        base_sales = store_size * 1000  # 1000 ØªÙˆÙ…Ø§Ù† Ø¯Ø± Ù…ØªØ± Ù…Ø±Ø¨Ø¹
        adjusted_sales = base_sales * (prediction_score / 100)
        
        results['sales_forecast'] = {
            'current_potential': round(adjusted_sales, 0),
            'optimized_potential': round(adjusted_sales * 1.3, 0),  # 30% Ø¨Ù‡Ø¨ÙˆØ¯
            'growth_rate': round(((adjusted_sales * 1.3) - adjusted_sales) / adjusted_sales * 100, 1)
        }
        
        # ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ Ø±Ø´Ø¯
        if traffic_score < 0.6:
            results['growth_opportunities'].append({
                'area': 'ØªØ±Ø§ÙÛŒÚ©',
                'potential': '20-30%',
                'description': 'Ø§ÙØ²Ø§ÛŒØ´ ØªØ±Ø§ÙÛŒÚ© Ù…Ø´ØªØ±ÛŒ'
            })
        
        if utilization_score < 0.7:
            results['growth_opportunities'].append({
                'area': 'Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ¶Ø§',
                'potential': '15-25%',
                'description': 'Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ¶Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡'
            })
        
        # ÙØ§Ú©ØªÙˆØ±Ù‡Ø§ÛŒ Ø±ÛŒØ³Ú©
        if traffic_score < 0.4:
            results['risk_factors'].append({
                'factor': 'ØªØ±Ø§ÙÛŒÚ© Ú©Ù…',
                'impact': 'high',
                'mitigation': 'Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ù‚ÙˆÛŒ'
            })
        
        if utilization_score < 0.5:
            results['risk_factors'].append({
                'factor': 'Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø§Ú©Ø§Ø±Ø¢Ù…Ø¯ Ø§Ø² ÙØ¶Ø§',
                'impact': 'medium',
                'mitigation': 'Ø¨Ø§Ø²Ø·Ø±Ø§Ø­ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù†'
            })
        
        log_memory_usage("Sales Prediction")
        return results
        
    except Exception as e:
        logger.error(f"Sales prediction error: {e}")
        return {'error': str(e)}

def generate_final_report(analysis, all_results, task_instance):
    """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ"""
    try:
        task_instance.update_state(
            state='PROGRESS',
            meta={'current': 6, 'total': 6, 'status': 'ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ...'}
        )
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ
        scores = []
        if 'layout_analysis' in all_results and 'efficiency_score' in all_results['layout_analysis']:
            scores.append(all_results['layout_analysis']['efficiency_score'])
        if 'customer_behavior' in all_results and 'behavior_score' in all_results['customer_behavior']:
            scores.append(all_results['customer_behavior']['behavior_score'])
        if 'traffic_analysis' in all_results and 'traffic_score' in all_results['traffic_analysis']:
            scores.append(all_results['traffic_analysis']['traffic_score'])
        if 'optimization' in all_results and 'optimization_score' in all_results['optimization']:
            scores.append(all_results['optimization']['optimization_score'])
        if 'sales_prediction' in all_results and 'prediction_score' in all_results['sales_prediction']:
            scores.append(all_results['sales_prediction']['prediction_score'])
        
        overall_score = sum(scores) / len(scores) if scores else 0
        
        # ØªØ¹ÛŒÛŒÙ† Ø±ØªØ¨Ù‡ Ú©Ù„ÛŒ
        if overall_score >= 80:
            grade = 'A'
            status = 'Ø¹Ø§Ù„ÛŒ'
        elif overall_score >= 70:
            grade = 'B'
            status = 'Ø®ÙˆØ¨'
        elif overall_score >= 60:
            grade = 'C'
            status = 'Ù…ØªÙˆØ³Ø·'
        elif overall_score >= 50:
            grade = 'D'
            status = 'Ø¶Ø¹ÛŒÙ'
        else:
            grade = 'F'
            status = 'Ø®ÛŒÙ„ÛŒ Ø¶Ø¹ÛŒÙ'
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
        all_recommendations = []
        for section, results in all_results.items():
            if isinstance(results, dict) and 'recommendations' in results:
                all_recommendations.extend(results['recommendations'])
        
        # Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
        critical_recommendations = [rec for rec in all_recommendations if rec.get('type') == 'critical']
        warning_recommendations = [rec for rec in all_recommendations if rec.get('type') == 'warning']
        info_recommendations = [rec for rec in all_recommendations if rec.get('type') == 'info']
        
        final_report = {
            'overall_score': round(overall_score, 1),
            'grade': grade,
            'status': status,
            'summary': {
                'store_name': analysis.store_info.store_name,
                'analysis_date': timezone.now().isoformat(),
                'total_recommendations': len(all_recommendations),
                'critical_issues': len(critical_recommendations),
                'optimization_opportunities': len([r for r in all_recommendations if 'optimization' in r.get('title', '').lower()])
            },
            'recommendations': {
                'critical': critical_recommendations[:5],  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 Ù…ÙˆØ±Ø¯
                'warning': warning_recommendations[:5],
                'info': info_recommendations[:5]
            },
            'next_steps': [
                'Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ',
                'Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§',
                'Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§',
                'ØªØ­Ù„ÛŒÙ„ Ù…Ø¬Ø¯Ø¯ Ù¾Ø³ Ø§Ø² 3 Ù…Ø§Ù‡'
            ]
        }
        
        log_memory_usage("Final Report Generation")
        return final_report
        
    except Exception as e:
        logger.error(f"Final report generation error: {e}")
        return {'error': str(e)}

# --- Background Tasks ---
@shared_task
def cleanup_old_files():
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
    try:
        cutoff_date = timezone.now() - timedelta(days=30)
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
        from django.core.files.storage import default_storage
        from django.core.files.base import ContentFile
        
        # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù†Ø·Ù‚ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
        
        logger.info("Old files cleanup completed")
        return {'status': 'success', 'cleaned_files': 0}
        
    except Exception as e:
        logger.error(f"File cleanup error: {e}")
        return {'status': 'error', 'error': str(e)}

@shared_task
def generate_analysis_report(analysis_id):
    """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ÛŒ"""
    try:
        analysis = StoreAnalysis.objects.get(id=analysis_id)

        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ PDF ÛŒØ§ Excel
        # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù†Ø·Ù‚ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯

        logger.info(f"Report generated for analysis {analysis_id}")
        return {'status': 'success', 'report_url': 'path/to/report'}

    except Exception as e:
        logger.error(f"Report generation error: {e}")
        return {'status': 'error', 'error': str(e)}

@shared_task
def start_real_time_analysis(analysis_id, store_data, user_id):
    """Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Real-time"""
    try:
        # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù†Ø·Ù‚ ØªØ­Ù„ÛŒÙ„ Real-time Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
        logger.info(f"Real-time analysis started for {analysis_id}")
        
        # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ØªØ­Ù„ÛŒÙ„
        time_module.sleep(5)  # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
        
        return {'status': 'success', 'analysis_id': analysis_id}
        
    except Exception as e:
        logger.error(f"Real-time analysis error: {e}")
        return {'status': 'error', 'error': str(e)}

# --- Health Check Tasks ---
@shared_task
def health_check():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ…"""
    try:
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        from django.db import connection
        with connection.cursor() as cursor:
            cursor.execute("SELECT 1")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´
        cache.set('health_check', 'ok', 60)
        cache_result = cache.get('health_check')
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø§ÙØ¸Ù‡
        process = psutil.Process()
        memory_usage = process.memory_info().rss / 1024 / 1024  # MB
        
        health_status = {
            'database': 'ok',
            'cache': 'ok' if cache_result == 'ok' else 'error',
            'memory_usage_mb': round(memory_usage, 2),
            'timestamp': timezone.now().isoformat()
        }
        
        logger.info(f"Health check completed: {health_status}")
        return health_status
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return {'status': 'error', 'error': str(e)}

# --- Monitoring Tasks ---
@shared_task
def monitor_system_performance():
    """Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ…"""
    try:
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¢Ù…Ø§Ø± Ø³ÛŒØ³ØªÙ…
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # Ø¢Ù…Ø§Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        from django.db import connection
        with connection.cursor() as cursor:
            cursor.execute("SELECT COUNT(*) FROM store_analysis_storeanalysis")
            total_analyses = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM store_analysis_storeanalysis WHERE status = 'processing'")
            processing_analyses = cursor.fetchone()[0]
        
        performance_data = {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'disk_percent': disk.percent,
            'total_analyses': total_analyses,
            'processing_analyses': processing_analyses,
            'timestamp': timezone.now().isoformat()
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯
        cache.set('system_performance', performance_data, 300)  # 5 Ø¯Ù‚ÛŒÙ‚Ù‡
        
        logger.info(f"System performance monitored: {performance_data}")
        return performance_data
        
    except Exception as e:
        logger.error(f"Performance monitoring error: {e}")
        return {'status': 'error', 'error': str(e)}


@shared_task
def process_analysis_with_ollama(analysis_id):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Ollama AI"""
    try:
        from .models import StoreAnalysis
        
        analysis = StoreAnalysis.objects.get(pk=analysis_id)
        analysis.status = 'processing'
        analysis.save()
        
        # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
        import time
        time.sleep(2)  # 2 Ø«Ø§Ù†ÛŒÙ‡ ØªØ§Ø®ÛŒØ±
        
        # Ù†ØªØ§ÛŒØ¬ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ
        results = {
            'analysis_text': 'ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø¨Ø§ Chidmano2 AI Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯',
            'source': 'ollama',
            'ai_provider': 'Chidmano2 AI',
            'package_type': 'advanced',
            'quality_score': 0.95,
            'confidence_score': 0.92,
            'free_plan': False
        }
        
        analysis.results = results
        analysis.status = 'completed'
        analysis.save()
        
        logger.info(f"Analysis {analysis_id} processed with Ollama AI")
        return {'status': 'success', 'message': 'ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Chidmano2 AI Ú©Ø§Ù…Ù„ Ø´Ø¯'}
        
    except Exception as e:
        logger.error(f"Ollama processing error: {e}")
        return {'status': 'error', 'error': str(e)}


@shared_task
def process_analysis_with_liara(analysis_id):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Liara AI"""
    try:
        from .models import StoreAnalysis
        
        analysis = StoreAnalysis.objects.get(pk=analysis_id)
        analysis.status = 'processing'
        analysis.save()
        
        # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
        import time
        time.sleep(2)  # 2 Ø«Ø§Ù†ÛŒÙ‡ ØªØ§Ø®ÛŒØ±
        
        # Ù†ØªØ§ÛŒØ¬ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ
        results = {
            'analysis_text': 'ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Chidmano1 AI Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯',
            'source': 'liara_ai',
            'ai_provider': 'Chidmano1 AI',
            'package_type': 'premium',
            'quality_score': 0.98,
            'confidence_score': 0.95,
            'free_plan': False
        }
        
        analysis.results = results
        analysis.status = 'completed'
        analysis.save()
        
        logger.info(f"Analysis {analysis_id} processed with Liara AI")
        return {'status': 'success', 'message': 'ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Chidmano1 AI Ú©Ø§Ù…Ù„ Ø´Ø¯'}
        
    except Exception as e:
        logger.error(f"Liara processing error: {e}")
        return {'status': 'error', 'error': str(e)}


@shared_task(name='store_analysis.send_review_reminders')
def send_review_reminders():
    """Ø§Ø±Ø³Ø§Ù„ Ø§ÛŒÙ…ÛŒÙ„ ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§"""
    from django.conf import settings
    from django.template.loader import render_to_string
    from .utils.notification import NotificationService
    
    try:
        # Ø¯Ø±ÛŒØ§ÙØª ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø§Ø±Ø³Ø§Ù„ Ø´ÙˆÙ†Ø¯
        reminders = ReviewReminder.objects.filter(
            status__in=['pending', 'scheduled'],
            email_sent=False,
            reminder_date__lte=timezone.now()
        )
        
        sent_count = 0
        failed_count = 0
        
        for reminder in reminders:
            try:
                user = reminder.user
                analysis = reminder.analysis
                
                # Ø³Ø§Ø®Øª Ù„ÛŒÙ†Ú© Ø±Ø²Ø±Ùˆ Ø¨Ø§ Ú©Ø¯ ØªØ®ÙÛŒÙ
                site_url = getattr(settings, 'SITE_URL', 'https://chidmano.liara.app')
                discount_code_str = reminder.discount_code.code if reminder.discount_code else ''
                booking_url = f"{site_url}/store/buy/?discount_code={discount_code_str}"
                
                # Ù…Ø­ØªÙˆØ§ÛŒ Ø§ÛŒÙ…ÛŒÙ„
                subject = f"ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒ - {analysis.store_name}"
                days_since = (timezone.now() - reminder.analysis_completed_at).days
                context = {
                    'user': user,
                    'analysis': analysis,
                    'discount_percentage': reminder.discount_percentage,
                    'discount_code': discount_code_str,
                    'booking_url': booking_url,
                    'site_url': site_url,
                    'analysis_date': reminder.analysis_completed_at,
                    'days_since_analysis': days_since,
                }
                
                # Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ Ø³Ø§Ø¯Ù‡
                plain_message = f"""
Ø³Ù„Ø§Ù… {user.get_full_name() or user.username},

ØªØ­Ù„ÛŒÙ„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {analysis.store_name} Ø´Ù…Ø§ {days_since} Ø±ÙˆØ² Ù¾ÛŒØ´ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯.

Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø±Ø´Ø¯ ÙØ¹Ù„ÛŒØŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¯Ø± 30 Ø±ÙˆØ² Ø¢ÛŒÙ†Ø¯Ù‡ Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.

ğŸ {reminder.discount_percentage}% ØªØ®ÙÛŒÙ ÙˆÛŒÚ˜Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ù…Ø§:
Ú©Ø¯ ØªØ®ÙÛŒÙ: {discount_code_str}

Ø¨Ø±Ø§ÛŒ Ø±Ø²Ø±Ùˆ Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ù„ÛŒÙ†Ú© Ø²ÛŒØ± Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯:
{booking_url}

Ø¨Ø§ ØªØ´Ú©Ø±
ØªÛŒÙ… Ú†ÛŒØ¯Ù…Ø§Ù†Ùˆ
"""
                
                success = NotificationService.send_email_notification(
                    to_email=user.email,
                    subject=subject,
                    message=plain_message,
                    html_template='store_analysis/emails/review_reminder.html',
                    context=context
                )
                
                if success:
                    reminder.mark_sent()
                    sent_count += 1
                    logger.info(f"Review reminder sent to {user.email} for analysis {analysis.id}")
                else:
                    failed_count += 1
                    logger.warning(f"Failed to send review reminder to {user.email}")
                    
            except Exception as e:
                failed_count += 1
                logger.error(f"Error sending review reminder {reminder.id}: {e}", exc_info=True)
        
        return {
            'status': 'success',
            'sent': sent_count,
            'failed': failed_count,
            'total': reminders.count()
        }
        
    except Exception as e:
        logger.error(f"Error in send_review_reminders task: {e}", exc_info=True)
        return {'status': 'error', 'message': str(e)} 