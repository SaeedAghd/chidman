"""
ูุฏุฑ ูพุดุฑูุชู ููุด ูุตููุน ุจุฑุง ฺุฏูุงูู
ุชุฑฺฉุจ Ollama ู ูุงุฑุง AI ุจุฑุง ุจูุชุฑู ุนููฺฉุฑุฏ
"""

import logging
from typing import Dict, Any, Optional
from django.conf import settings
from django.core.cache import cache
import time

from .liara_ai_service import LiaraAIService
from ..ai_analysis import StoreAnalysisAI

logger = logging.getLogger(__name__)

class AdvancedAIManager:
    """ูุฏุฑ ูพุดุฑูุชู ููุด ูุตููุน"""
    
    def __init__(self):
        self.liara_ai = LiaraAIService()
        self.ollama_ai = StoreAnalysisAI()
        self.use_liara = getattr(settings, 'USE_LIARA_AI', True)
        self.fallback_to_ollama = getattr(settings, 'FALLBACK_TO_OLLAMA', True)
    
    def analyze_store_advanced(self, store_data: Dict[str, Any]) -> Dict[str, Any]:
        """ุชุญูู ูพุดุฑูุชู ูุฑูุดฺฏุงู ุจุง ุจูุชุฑู AI"""
        
        store_name = store_data.get('store_name', 'ูุฑูุดฺฏุงู')
        logger.info(f"ุดุฑูุน ุชุญูู ูพุดุฑูุชู ุจุฑุง ูุฑูุดฺฏุงู: {store_name}")
        
        # ุงุณุชุฑุงุชฺ ุชุญูู
        if self.use_liara:
            try:
                # ุงูููุช ุจุง ูุงุฑุง AI
                result = self._analyze_with_liara(store_data)
                if result and not result.get('error'):
                    logger.info(f"ุชุญูู ูููู ุจุง ูุงุฑุง AI ุจุฑุง: {store_name}")
                    return result
            except Exception as e:
                logger.warning(f"ุฎุทุง ุฏุฑ ูุงุฑุง AI: {e}")
        
        # Fallback ุจู Ollama
        if self.fallback_to_ollama:
            try:
                result = self._analyze_with_ollama(store_data)
                if result:
                    logger.info(f"ุชุญูู ูููู ุจุง Ollama ุจุฑุง: {store_name}")
                    return result
            except Exception as e:
                logger.warning(f"ุฎุทุง ุฏุฑ Ollama: {e}")
        
        # Fallback ููุง
        return self._get_emergency_analysis(store_data)
    
    def _analyze_with_liara(self, store_data: Dict[str, Any]) -> Dict[str, Any]:
        """ุชุญูู ุจุง ูุงุฑุง AI"""
        try:
            result = self.liara_ai.get_ai_insights(store_data)
            
            # ุงุถุงูู ฺฉุฑุฏู metadata
            result['ai_provider'] = 'liara'
            result['analysis_quality'] = 'premium'
            result['models_used'] = result.get('ai_models_used', [])
            
            return result
        except Exception as e:
            logger.error(f"ุฎุทุง ุฏุฑ ุชุญูู ูุงุฑุง: {e}")
            return None
    
    def _analyze_with_ollama(self, store_data: Dict[str, Any]) -> Dict[str, Any]:
        """ุชุญูู ุจุง Ollama"""
        try:
            result = self.ollama_ai.generate_detailed_analysis(store_data)
            
            # ุชุจุฏู ุจู ูุฑูุช ฺฉุณุงู
            formatted_result = {
                'final_report': result.get('analysis_text', 'ุชุญูู ุจุง Ollama ุงูุฌุงู ุดุฏ'),
                'detailed_analyses': {
                    'ollama_analysis': {
                        'type': 'ollama_analysis',
                        'content': result.get('analysis_text', ''),
                        'model': 'llama3.2'
                    }
                },
                'store_info': store_data,
                'analysis_timestamp': time.time(),
                'ai_provider': 'ollama',
                'analysis_quality': 'standard',
                'models_used': ['llama3.2']
            }
            
            return formatted_result
        except Exception as e:
            logger.error(f"ุฎุทุง ุฏุฑ ุชุญูู Ollama: {e}")
            return None
    
    def _get_emergency_analysis(self, store_data: Dict[str, Any]) -> Dict[str, Any]:
        """ุชุญูู ุงุถุทุฑุงุฑ"""
        store_name = store_data.get('store_name', 'ูุฑูุดฺฏุงู')
        
        return {
            'final_report': f"""
            ## ๐ฏ ุชุญูู ูุฑูุดฺฏุงู {store_name}
            
            ### ๐ ูุถุนุช ฺฉู
            ูุฑูุดฺฏุงู ุดูุง ุฏุฑ ุญุงู ุชุญูู ุงุณุช. ูุทูุงู ฺูุฏ ุฏููู ุตุจุฑ ฺฉูุฏ ู ูุฌุฏุฏุงู ุชูุงุด ฺฉูุฏ.
            
            ### ๐ก ุชูุตูโูุง ฺฉู
            1. **ููุฑูพุฑุฏุงุฒ ููุงุณุจ**: ุงุฒ ููุฑ ุทุจุน ู ูุตููุน ูุชุนุงุฏู ุงุณุชูุงุฏู ฺฉูุฏ
            2. **ฺุฏูุงู ููุทู**: ูุญุตููุงุช ุฑุง ุจุฑ ุงุณุงุณ ููุทู ุฎุฑุฏ ฺุฏูุงู ฺฉูุฏ
            3. **ุฑูฺฏโุจูุฏ ููุงููฺฏ**: ุงุฒ ุฑูฺฏโูุง ููุงููฺฏ ู ุฌุฐุงุจ ุงุณุชูุงุฏู ฺฉูุฏ
            4. **ูุถุง ฺฉุงู**: ูุถุง ฺฉุงู ุจุฑุง ุญุฑฺฉุช ูุดุชุฑุงู ูุฑุงูู ฺฉูุฏ
            5. **ููุงุด ูุญุตููุงุช**: ูุญุตููุงุช ุฑุง ุจู ุตูุฑุช ุฌุฐุงุจ ููุงุด ุฏูุฏ
            
            ### ๐ ูพุชุงูุณู ุงูุฒุงุด ูุฑูุด
            ุจุง ุจูููโุณุงุฒ ฺุฏูุงูุ ูโุชูุงูุฏ ุชุง 30% ุงูุฒุงุด ูุฑูุด ุฏุงุดุชู ุจุงุดุฏ.
            """,
            'detailed_analyses': {},
            'store_info': store_data,
            'analysis_timestamp': time.time(),
            'ai_provider': 'emergency',
            'analysis_quality': 'basic',
            'models_used': ['emergency_fallback'],
            'error': 'ุฎุทุง ุฏุฑ ุณุฑูุณโูุง AI'
        }
    
    def get_analysis_status(self, store_data: Dict[str, Any]) -> Dict[str, Any]:
        """ุจุฑุฑุณ ูุถุนุช ุชุญูู"""
        
        # ุจุฑุฑุณ cache
        cache_key = f"analysis_status_{hash(str(store_data))}"
        status = cache.get(cache_key)
        
        if not status:
            status = {
                'status': 'pending',
                'progress': 0,
                'message': 'ุฏุฑ ุญุงู ุขูุงุฏูโุณุงุฒ ุชุญูู...'
            }
            cache.set(cache_key, status, 300)  # 5 ุฏููู
        
        return status
    
    def start_advanced_analysis(self, store_data: Dict[str, Any]) -> Dict[str, Any]:
        """ุดุฑูุน ุชุญูู ูพุดุฑูุชู"""
        
        store_name = store_data.get('store_name', 'ูุฑูุดฺฏุงู')
        
        # ุชูุธู ูุถุนุช
        cache_key = f"analysis_status_{hash(str(store_data))}"
        status = {
            'status': 'processing',
            'progress': 10,
            'message': f'ุดุฑูุน ุชุญูู ูพุดุฑูุชู ุจุฑุง {store_name}...'
        }
        cache.set(cache_key, status, 300)
        
        # ุดุฑูุน ุชุญูู
        try:
            result = self.analyze_store_advanced(store_data)
            
            # ุจูโุฑูุฒุฑุณุงู ูุถุนุช
            status.update({
                'status': 'completed',
                'progress': 100,
                'message': 'ุชุญูู ุจุง ููููุช ุชฺฉูู ุดุฏ!',
                'result': result
            })
            cache.set(cache_key, status, 3600)  # 1 ุณุงุนุช
            
            return result
            
        except Exception as e:
            logger.error(f"ุฎุทุง ุฏุฑ ุชุญูู ูพุดุฑูุชู: {e}")
            
            # ุจูโุฑูุฒุฑุณุงู ูุถุนุช ุฎุทุง
            status.update({
                'status': 'failed',
                'progress': 0,
                'message': f'ุฎุทุง ุฏุฑ ุชุญูู: {str(e)}'
            })
            cache.set(cache_key, status, 300)
            
            return self._get_emergency_analysis(store_data)
    
    def get_ai_capabilities(self) -> Dict[str, Any]:
        """ุฏุฑุงูุช ูุงุจูุชโูุง AI"""
        
        return {
            'liara_ai': {
                'available': self.use_liara,
                'models': [
                    'openai/gpt-4.1',  # ุฌุฏุฏุชุฑู ู ูุฏุฑุชููุฏุชุฑู ูุฏู
                    'openai/gpt-4-turbo',
                    'openai/gpt-4o',
                    'claude-3-opus',
                    'claude-3-sonnet'
                ],
                'capabilities': [
                    'ุชุญูู ุฌุงูุน ูุฑูุดฺฏุงู ุจุง GPT-4.1',
                    'ุชุญูู ุทุฑุงุญ ุญุฑููโุง ูพุดุฑูุชู',
                    'ุชุญูู ุฑูุงูุดูุงุณ ูุดุชุฑ ุนูู',
                    'ุชุญูู ุจุงุฒุงุฑุงุจ ุงุณุชุฑุงุชฺฺฉ',
                    'ุจูููโุณุงุฒ ุนููฺฉุฑุฏ ููุดููุฏ',
                    'ุจูุดโูุง ูพุดุฑูุชู ฺฉุณุจโูฺฉุงุฑ',
                    'ุชูุตูโูุง ุดุฎุตโุณุงุฒ ุดุฏู'
                ]
            },
            'ollama_ai': {
                'available': self.fallback_to_ollama,
                'models': ['llama3.2'],
                'capabilities': [
                    'ุชุญูู ูพุงู ูุฑูุดฺฏุงู',
                    'ุชูุตูโูุง ฺฉู'
                ]
            },
            'hybrid_mode': {
                'enabled': True,
                'strategy': 'liara_first_ollama_fallback',
                'benefits': [
                    'ูุงุจูุช ุงุทููุงู ุจุงูุง',
                    'ุชุญููโูุง ูพุดุฑูุชู',
                    'ูพุดุชุจุงู ุงุฒ ฺูุฏู ูุฏู',
                    'Fallback ุฎูุฏฺฉุงุฑ'
                ]
            }
        }
